{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0894d108",
   "metadata": {},
   "source": [
    "# Primera entrega\n",
    "Luis Montenegro - 21699<br>\n",
    "Javier Prado - 21486<br>\n",
    "Bryan España<br>\n",
    "Ángel Herrarte<br>\n",
    "\n",
    "\n",
    "## Limpieza de datos\n",
    "En esta entrega se hará una recolección y limpieza de datos solamente. No habrá modelado ni análisis. \n",
    "<br>Solamente velar por la integridad, coherencia y cohesión del conjunto de datos.facilidad de manejo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff6290c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "ERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements.txt'\n"
     ]
    }
   ],
   "source": [
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa43cd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a3fcc9",
   "metadata": {},
   "source": [
    "### Conversión de .xls a .csv\n",
    "\n",
    "Debido a que los archivos crudos descargados desde la página del Mineduc vienen en formato .xls formateado como .html tenemos que hacer cambio de eso.<br>\n",
    "Esto por múltiples razones:\n",
    "- Mejorar la estructura de los datos.\n",
    "- Preservar solamente la información requerida (los .xls almaceban más páginas e información irrelevante)\n",
    "- Mayor falicidad de usar y obtener los datos si están en formato delimitado por comas.\n",
    "- Tener datos más ligeros ya que no acarreamos con mucha información innecesaria. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "456d91fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_html_to_csv(input_path: str, output_path: str=None, tables_to_convert: list[int] = [9]) -> None:\n",
    "    '''\n",
    "    Turns .html files into .csv files\n",
    "\n",
    "    Params:\n",
    "        input_path: where the .html files are stored\n",
    "        output_path: where the .csv will be stored\n",
    "    Returns:\n",
    "        out: None\n",
    "    '''\n",
    "    if output_path is None:\n",
    "        output_path = input_path \n",
    "    else:\n",
    "        os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "    for filename in tqdm(os.listdir(input_path), desc='Converting Files'):\n",
    "        if filename.endswith('.xls'):\n",
    "            html_path:str = os.path.join(input_path, filename)\n",
    "            base_name:str = os.path.splitext(filename)[0]\n",
    "            # try to read the excel and copy it into a .csv file\n",
    "            try: \n",
    "                tables:list[pd.DataFrame] = pd.read_html(html_path)\n",
    "                # if no tables, skip\n",
    "                if not tables:\n",
    "                    print(f\"No tables found in '{filename}'\")\n",
    "                    continue\n",
    "\n",
    "                # convert each of the tables selected to .csv\n",
    "                for i, df in enumerate(tables):\n",
    "                    if i in tables_to_convert:\n",
    "                        csv_filename = f\"{base_name}_table{i}.csv\"\n",
    "                        csv_path = os.path.join(output_path, csv_filename)\n",
    "                        df.to_csv(csv_path, index=False)\n",
    "                        print(f\"Converted '{filename}' to '{csv_filename}'\") # print all of the transformations of the .xls to its respective .csv files\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file '{filename}' : {e}\")\n",
    "\n",
    "def remove_files(path: str, extension: str) -> None:\n",
    "    '''\n",
    "    Removes all files that have a certain extension\n",
    "    Input:\n",
    "        path: folder path where .xls to be removed are stored\n",
    "    '''\n",
    "    file_list: list[str] = os.listdir(path)\n",
    "    if not file_list:\n",
    "        print(f\"No files removed since there was none found: '{path}'\")\n",
    "        return\n",
    "\n",
    "    for filename in file_list:\n",
    "    \n",
    "        if filename.endswith(extension):\n",
    "            try:\n",
    "                xls_path = os.path.join(path, filename)\n",
    "                os.remove(xls_path)\n",
    "            except Exception as e:\n",
    "                print(f\"Error removing file '{filename}' : {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f249f5f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting Files: 0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "# start by turning .xls files to .csv files\n",
    "transform_html_to_csv(input_path=\"../Dataset_raw\", output_path=\"../Dataset_cleaned\", tables_to_convert=[9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b81a4e90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No files removed since there was none found: '../Dataset_raw'\n"
     ]
    }
   ],
   "source": [
    "# remove .xls files if necessary\n",
    "remove_files(path=\"../Dataset_raw\", extension='.xls')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7e543e",
   "metadata": {},
   "source": [
    "### Plan para limpieza de datos\n",
    "Posteriormente a la transformación de formato y filtración a solo datos crudos de nuestro interés, procederemos a limpiar los datos en sí. <br>\n",
    "1. Verificaremos si existe datos NA o Null en los archivos. Llenaremos los datos faltantes con datos artificiales ya que no podemos eliminar ningún registro <br>\n",
    "debido a que si eliminamos alguno, sería una sede faltante. Cosa que afectaría de gran manera el conjunto de datos.\n",
    "2. Homogenizaremos la información. Es decir, que todos los archivos tengan el mismo formato i.e. todos los nombres estén escritos igual, los apellidos que tienen tildes se escriban igual en cada archivo,\n",
    "que las palabras vengan o solo en mayúsculas, solo en minúsculas, etc.\n",
    "3. Identificaremos las columnas que más trabajo de reajuste necesiten.\n",
    "- Nombres (Supervisor, Director, Establecimiento, Sector): debido a que puede que exista apellidos iguales pero escritos distinto y eso genere problemas a la hora de verlos. Digamos, puede que Hernández aparezca con tilde en unos registros y en otros no. También es bueno verificar que estén escritos todos o en mayúsculas o en minúsculas (o todos iguales), ya que digamos, si tenemos Santa rosa en un registro y Santa Rosa en otro, a la hora de hacer un encoding estos resultarán con dos valores distintos. \n",
    "- Dirección: Asegurarnos que las direcciones lleven una estructura similar.\n",
    "- Códigos: Verificar que los códigos de los registros sean únicos y evitar tener repetidos.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6208bf",
   "metadata": {},
   "source": [
    "El conjunto de datos corresponde a los establecimientos educativos de Guatemala que llegan hasta el nivel diversificado. Está organizado en 85 archivos CSV, uno por cada departamento y municipio del país.\n",
    "\n",
    "Cada archivo contiene información detallada de cada establecimiento, como el nombre del centro, su ubicación, datos de contacto, modalidad de enseñanza y otros datos administrativos.\n",
    "\n",
    "\n",
    "Los archivos csv estan clasificados por departamentos de Guatemala. Las variables que contiene esta dataset son las siguientes sumando un total de 17 variables a analizar.\n",
    "\n",
    "- CODIGO: código único del establecimiento\n",
    "\n",
    "- DISTRITO: distrito educativo\n",
    "\n",
    "- DEPARTAMENTO: nombre del departamento\n",
    "\n",
    "- MUNICIPIO: municipio donde se ubica\n",
    "\n",
    "- ESTABLECIMIENTO: nombre del centro educativo\n",
    "\n",
    "- DIRECCION: dirección física\n",
    "\n",
    "- TELEFONO: número de contacto\n",
    "\n",
    "- SUPERVISOR: nombre del supervisor\n",
    "\n",
    "- DIRECTOR: nombre del director\n",
    "\n",
    "- NIVEL: nivel educativo (ej. Básico, Diversificado)\n",
    "\n",
    "- SECTOR: sector oficial o privado\n",
    "\n",
    "- AREA: área urbana o rural\n",
    "\n",
    "- STATUS: estado del centro (ej. Abierta)\n",
    "\n",
    "- MODALIDAD: modalidad lingüística (ej. Monolingüe, Bilingüe)\n",
    "\n",
    "- JORNADA: jornada de estudio (ej. Matutina, Vespertina)\n",
    "\n",
    "- PLAN: plan educativo (ej. Diario, Fin de semana)\n",
    "\n",
    "- DEPARTAMENTAL: nombre del departamento de adscripción"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0657006d",
   "metadata": {},
   "source": [
    "### Encontrar Null o NA en cualesquiera archivos\n",
    "Comenzaremos viendo si existe cualesquiera archivos con datos faltantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e5db5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path: str = \"../Dataset_cleaned\" # define folder path as a variable for easier handling\n",
    "report_path: str = \"../Data_null_report\"\n",
    "\n",
    "def get_null_stats(path:str) -> None:\n",
    "        '''\n",
    "        Prints the columns with most null counts and the files with most nulls\n",
    "        '''\n",
    "        df: pd.DataFrame = pd.read_csv(path)\n",
    "        print(\"\\n Top columns with the most nulls:\")\n",
    "        top_columns = df.groupby('column')['nulls'].sum().sort_values(ascending=False).head(10)\n",
    "        print(top_columns)\n",
    "\n",
    "        print(\"\\n Files with the most nulls:\")\n",
    "        top_files = df.groupby('file_name')['nulls'].sum().sort_values(ascending=False).head(10)\n",
    "        print(top_files)\n",
    "\n",
    "def count_null_instances(path:str, report_name:str = \"null_report.csv\") -> None:\n",
    "    '''\n",
    "    Counts how many missing instances are found per column per .csv and saves it into a .csv\n",
    "    '''\n",
    "    files: list[str] = os.listdir(path=path)\n",
    "    report_file: str = os.path.join(report_path, report_name)\n",
    "    os.makedirs(name=report_path, exist_ok=True) # make a new directory to save this information\n",
    "    data:list[dict] = []\n",
    "    for file in tqdm(files, desc=\"Counting nulls..\"):\n",
    "        file_path:str = os.path.join(path, file)\n",
    "        try:\n",
    "            df: pd.DataFrame = pd.read_csv(file_path, header=1) # read second line as header, first one are just integers\n",
    "            null_counts = df.isnull().sum() # register nulls found in the df per column\n",
    "            for col, count in null_counts.items():\n",
    "                data.append({\n",
    "                    \"file_name\": file,\n",
    "                    \"column\": col,\n",
    "                    \"nulls\": count\n",
    "                })\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file} : {e}\")\n",
    "    \n",
    "    report_df: pd.DataFrame = pd.DataFrame(data=data)\n",
    "    report_df.to_csv(report_file, index=False) # not necessary to save it as a .csv\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca41853",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Counting nulls..: 100%|██████████| 85/85 [00:00<00:00, 222.65it/s]\n"
     ]
    }
   ],
   "source": [
    "count_null_instances(path=dataset_path) # count nulls found per column in each .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6e9e502d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Top columns with the most nulls:\n",
      "column\n",
      "TELEFONO           4404\n",
      "DIRECTOR            200\n",
      "DIRECCION           107\n",
      "PLAN                 84\n",
      "CODIGO               84\n",
      "AREA                 84\n",
      "DEPARTAMENTO         84\n",
      "ESTABLECIMIENTO      84\n",
      "DISTRITO             84\n",
      "JORNADA              84\n",
      "Name: nulls, dtype: int64\n",
      "\n",
      " Files with the most nulls:\n",
      "file_name\n",
      "alta_verapaz_primaria_table9.csv        650\n",
      "alta_verapaz_preprimaria_table9.csv     393\n",
      "quiche_primaria_table9.csv              351\n",
      "peten_primaria_table9.csv               328\n",
      "jutiapa_primaria_table9.csv             294\n",
      "huehuetenango_primaria_table9.csv       275\n",
      "san_marcos_primaria_table9.csv          242\n",
      "santa_rosa_primaria_table9.csv          242\n",
      "baja_verapaz_primaria_table9.csv        183\n",
      "huehuetenango_preprimaria_table9.csv    155\n",
      "Name: nulls, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "get_null_stats(path=\"../Data_null_report/null_report.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e19e2a",
   "metadata": {},
   "source": [
    "Como podemos observar, las columnas con mayor recuento de nulos son de TELEFONO, DIRECTOR y DIRECCION. Mientras que los archivos con más nulos son altaverapaz con primaria y preprimaria. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd01f2bd",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
