{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0894d108",
   "metadata": {},
   "source": [
    "# Primera entrega\n",
    "Luis Montenegro - 21699<br>\n",
    "Javier Prado - 21486<br>\n",
    "Bryan Espa√±a<br>\n",
    "√Ångel Herrarte<br>\n",
    "\n",
    "\n",
    "## Limpieza de datos\n",
    "En esta entrega se har√° una recolecci√≥n y limpieza de datos solamente. No habr√° modelado ni an√°lisis. \n",
    "<br>Solamente velar por la integridad, coherencia y cohesi√≥n del conjunto de datos.facilidad de manejo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ff6290c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa43cd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import unicodedata\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a3fcc9",
   "metadata": {},
   "source": [
    "### Conversi√≥n de .xls a .csv\n",
    "\n",
    "Debido a que los archivos crudos descargados desde la p√°gina del Mineduc vienen en formato .xls formateado como .html tenemos que hacer cambio de eso.<br>\n",
    "Esto por m√∫ltiples razones:\n",
    "- Mejorar la estructura de los datos.\n",
    "- Preservar solamente la informaci√≥n requerida (los .xls almaceban m√°s p√°ginas e informaci√≥n irrelevante)\n",
    "- Mayor falicidad de usar y obtener los datos si est√°n en formato delimitado por comas.\n",
    "- Tener datos m√°s ligeros ya que no acarreamos con mucha informaci√≥n innecesaria. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "456d91fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_html_to_csv(input_path: str, output_path: str=None, tables_to_convert: list[int] = [9]) -> None:\n",
    "    '''\n",
    "    Turns .html files into .csv files\n",
    "\n",
    "    Params:\n",
    "        input_path: where the .html files are stored\n",
    "        output_path: where the .csv will be stored\n",
    "    Returns:\n",
    "        out: None\n",
    "    '''\n",
    "    if output_path is None:\n",
    "        output_path = input_path \n",
    "    else:\n",
    "        os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "    for filename in tqdm(os.listdir(input_path), desc='Converting Files'):\n",
    "        if filename.endswith('.xls'):\n",
    "            html_path:str = os.path.join(input_path, filename)\n",
    "            base_name:str = os.path.splitext(filename)[0]\n",
    "            # try to read the excel and copy it into a .csv file\n",
    "            try: \n",
    "                tables:list[pd.DataFrame] = pd.read_html(html_path)\n",
    "                # if no tables, skip\n",
    "                if not tables:\n",
    "                    print(f\"No tables found in '{filename}'\")\n",
    "                    continue\n",
    "\n",
    "                # convert each of the tables selected to .csv\n",
    "                for i, df in enumerate(tables):\n",
    "                    if i in tables_to_convert:\n",
    "                        csv_filename = f\"{base_name}_table{i}.csv\"\n",
    "                        csv_path = os.path.join(output_path, csv_filename)\n",
    "                        df.to_csv(csv_path, index=False)\n",
    "                        print(f\"Converted '{filename}' to '{csv_filename}'\") # print all of the transformations of the .xls to its respective .csv files\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file '{filename}' : {e}\")\n",
    "\n",
    "def remove_files(path: str, extension: str) -> None:\n",
    "    '''\n",
    "    Removes all files that have a certain extension\n",
    "    Input:\n",
    "        path: folder path where .xls to be removed are stored\n",
    "    '''\n",
    "    file_list: list[str] = os.listdir(path)\n",
    "    if not file_list:\n",
    "        print(f\"No files removed since there was none found: '{path}'\")\n",
    "        return\n",
    "\n",
    "    for filename in file_list:\n",
    "    \n",
    "        if filename.endswith(extension):\n",
    "            try:\n",
    "                xls_path = os.path.join(path, filename)\n",
    "                os.remove(xls_path)\n",
    "            except Exception as e:\n",
    "                print(f\"Error removing file '{filename}' : {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f249f5f7",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../Dataset_raw'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# start by turning .xls files to .csv files\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mtransform_html_to_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_path\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m../Dataset_raw\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m../Dataset_cleaned\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtables_to_convert\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m9\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 16\u001b[39m, in \u001b[36mtransform_html_to_csv\u001b[39m\u001b[34m(input_path, output_path, tables_to_convert)\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     14\u001b[39m     os.makedirs(output_path, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_path\u001b[49m\u001b[43m)\u001b[49m, desc=\u001b[33m'\u001b[39m\u001b[33mConverting Files\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m     17\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m filename.endswith(\u001b[33m'\u001b[39m\u001b[33m.xls\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m     18\u001b[39m         html_path:\u001b[38;5;28mstr\u001b[39m = os.path.join(input_path, filename)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '../Dataset_raw'"
     ]
    }
   ],
   "source": [
    "# start by turning .xls files to .csv files\n",
    "transform_html_to_csv(input_path=\"../Dataset_raw\", output_path=\"../Dataset_cleaned\", tables_to_convert=[9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81a4e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove .xls files if necessary\n",
    "remove_files(path=\"../Dataset_raw\", extension='.xls')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65016283",
   "metadata": {},
   "source": [
    "### Estandarizar la estructura de los .csv\n",
    "Es importante que los .csv con los que estamos trabajando tengan una misma estructura. Por lo tanto, debemos asegurarnos que todos sean iguales. <br>\n",
    "Esto lo hacemos con la finalidad de evitar errores al limpiar los datos m√°s adelante o al concatenarlos al final. \n",
    "\n",
    "Queremos que todos los .csv tengan 17 t√≠tulos de columnas en orden tipo:<br>\n",
    "CODIGO,DISTRITO,DEPARTAMENTO,MUNICIPIO,ESTABLECIMIENTO,DIRECCION,TELEFONO,SUPERVISOR,DIRECTOR,NIVEL,SECTOR,AREA,STATUS,MODALIDAD,JORNADA,PLAN,DEPARTAMENTAL<br>\n",
    "\n",
    "Otra cosa a considerar es eliminar la primera fila y reajustar √≠ndices. Debido a que la primera fila son puros n√∫meros en vez de los nombres de las columnas. Al igual que debemos\n",
    "eliminar la √∫ltima fila que es todos nulos. \n",
    "\n",
    "Para los que ten√≠an error de formato o formato diferente solamente copiamos los datos en un .csv ya que usualmente eso se debe a que solamente hab√≠a 1 establecimiento encontrado.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3558893f",
   "metadata": {},
   "outputs": [],
   "source": [
    "COLUMNS = [\"CODIGO\",\"DISTRITO\",\"DEPARTAMENTO\",\"MUNICIPIO\",\"ESTABLECIMIENTO\",\"DIRECCION\",\"TELEFONO\",\"SUPERVISOR\",\"DIRECTOR\",\"NIVEL\",\"SECTOR\",\"AREA\",\"STATUS\",\"MODALIDAD\",\"JORNADA\",\"PLAN\",\"DEPARTAMENTAL\"]\n",
    "\n",
    "def clean_headers_and_trailers(path: str) -> None:\n",
    "    '''\n",
    "    Removes first row and reindexes the data within.\n",
    "    Also removes the last row that contains only nulls\n",
    "    '''\n",
    "    for file in tqdm(os.listdir(path), desc=\"Converting files\"):\n",
    "        if file.endswith('.csv'):\n",
    "            try:\n",
    "                f_path = os.path.join(path, file) # path to the file\n",
    "                df: pd.DataFrame = pd.read_csv(f_path)\n",
    "                \n",
    "                # drop first row and reindex if the first row are digits and not columns\n",
    "                if all(col.strip().isdigit() for col in df.columns):\n",
    "                    df.columns = df.iloc[0]\n",
    "                    df = df[1:].reset_index(drop=True)\n",
    "\n",
    "                # remove last row if only nulls are found\n",
    "                if df.tail(1).isnull().all(axis=1).iloc[0]:\n",
    "                    df = df.iloc[:-1]\n",
    "                    print(f\"Removed last row: {file}\")\n",
    "                \n",
    "                df.to_csv(f_path, index=False)  \n",
    "                \n",
    "                print(f\"Finished: {file}\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing '{file}': {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99e3f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean headers and trailers\n",
    "# clean_headers_and_trailers(path = \"../Dataset_cleaned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7e543e",
   "metadata": {},
   "source": [
    "### Plan para limpieza de datos\n",
    "Posteriormente a la transformaci√≥n de formato y filtraci√≥n a solo datos crudos de nuestro inter√©s, procederemos a limpiar los datos en s√≠. <br>\n",
    "1. Verificaremos si existe datos NA o Null en los archivos. Llenaremos los datos faltantes con datos artificiales ya que no podemos eliminar ning√∫n registro <br>\n",
    "debido a que si eliminamos alguno, ser√≠a una sede faltante. Cosa que afectar√≠a de gran manera el conjunto de datos.\n",
    "2. Homogenizaremos la informaci√≥n. Es decir, que todos los archivos tengan el mismo formato i.e. todos los nombres est√©n escritos igual, los apellidos que tienen tildes se escriban igual en cada archivo,\n",
    "que las palabras vengan o solo en may√∫sculas, solo en min√∫sculas, etc.\n",
    "3. Identificaremos las columnas que m√°s trabajo de reajuste necesiten.\n",
    "- Nombres (Supervisor, Director, Establecimiento, Sector): debido a que puede que exista apellidos iguales pero escritos distinto y eso genere problemas a la hora de verlos. Digamos, puede que Hern√°ndez aparezca con tilde en unos registros y en otros no. Tambi√©n es bueno verificar que est√©n escritos todos o en may√∫sculas o en min√∫sculas (o todos iguales), ya que digamos, si tenemos Santa rosa en un registro y Santa Rosa en otro, a la hora de hacer un encoding estos resultar√°n con dos valores distintos. \n",
    "- Direcci√≥n: Asegurarnos que las direcciones lleven una estructura similar.\n",
    "- C√≥digos: Verificar que los c√≥digos de los registros sean √∫nicos y evitar tener repetidos.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6208bf",
   "metadata": {},
   "source": [
    "El conjunto de datos corresponde a los establecimientos educativos de Guatemala que llegan hasta el nivel diversificado. Est√° organizado en 85 archivos CSV, uno por cada departamento y municipio del pa√≠s.\n",
    "\n",
    "Cada archivo contiene informaci√≥n detallada de cada establecimiento, como el nombre del centro, su ubicaci√≥n, datos de contacto, modalidad de ense√±anza y otros datos administrativos.\n",
    "\n",
    "\n",
    "Los archivos csv estan clasificados por departamentos de Guatemala. Las variables que contiene esta dataset son las siguientes sumando un total de 17 variables a analizar.\n",
    "\n",
    "- CODIGO: c√≥digo √∫nico del establecimiento\n",
    "\n",
    "- DISTRITO: distrito educativo\n",
    "\n",
    "- DEPARTAMENTO: nombre del departamento\n",
    "\n",
    "- MUNICIPIO: municipio donde se ubica\n",
    "\n",
    "- ESTABLECIMIENTO: nombre del centro educativo\n",
    "\n",
    "- DIRECCION: direcci√≥n f√≠sica\n",
    "\n",
    "- TELEFONO: n√∫mero de contacto\n",
    "\n",
    "- SUPERVISOR: nombre del supervisor\n",
    "\n",
    "- DIRECTOR: nombre del director\n",
    "\n",
    "- NIVEL: nivel educativo (ej. B√°sico, Diversificado)\n",
    "\n",
    "- SECTOR: sector oficial o privado\n",
    "\n",
    "- AREA: √°rea urbana o rural\n",
    "\n",
    "- STATUS: estado del centro (ej. Abierta)\n",
    "\n",
    "- MODALIDAD: modalidad ling√º√≠stica (ej. Monoling√ºe, Biling√ºe)\n",
    "\n",
    "- JORNADA: jornada de estudio (ej. Matutina, Vespertina)\n",
    "\n",
    "- PLAN: plan educativo (ej. Diario, Fin de semana)\n",
    "\n",
    "- DEPARTAMENTAL: nombre del departamento de adscripci√≥n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0657006d",
   "metadata": {},
   "source": [
    "### Encontrar Null o NA en cualesquiera archivos\n",
    "Comenzaremos viendo si existe cualesquiera archivos con datos faltantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37e5db5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path: str = \"../Dataset_cleaned\" # define folder path as a variable for easier handling\n",
    "report_path: str = \"../Data_null_report\"\n",
    "\n",
    "def get_null_stats(path:str) -> None:\n",
    "        '''\n",
    "        Prints the columns with most null counts and the files with most nulls\n",
    "        '''\n",
    "        df: pd.DataFrame = pd.read_csv(path)\n",
    "        print(\"\\n Top columns with the most nulls:\")\n",
    "        top_columns = df.groupby('column')['nulls'].sum().sort_values(ascending=False).head(10)\n",
    "        print(top_columns)\n",
    "\n",
    "        print(\"\\n Files with the most nulls:\")\n",
    "        top_files = df.groupby('file_name')['nulls'].sum().sort_values(ascending=False).head(10)\n",
    "        print(top_files)\n",
    "\n",
    "def count_null_instances(path:str, report_name:str = \"null_report.csv\") -> None:\n",
    "    '''\n",
    "    Counts how many missing instances are found per column per .csv and saves it into a .csv\n",
    "    '''\n",
    "    files: list[str] = os.listdir(path=path)\n",
    "    report_file: str = os.path.join(report_path, report_name)\n",
    "    os.makedirs(name=report_path, exist_ok=True) # make a new directory to save this information\n",
    "    data:list[dict] = []\n",
    "    for file in tqdm(files, desc=\"Counting nulls..\"):\n",
    "        file_path:str = os.path.join(path, file)\n",
    "        try:\n",
    "            df: pd.DataFrame = pd.read_csv(file_path) \n",
    "            null_counts = df.isnull().sum() # register nulls found in the df per column\n",
    "            for col, count in null_counts.items():\n",
    "                data.append({\n",
    "                    \"file_name\": file,\n",
    "                    \"column\": col,\n",
    "                    \"nulls\": count\n",
    "                })\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file} : {e}\")\n",
    "    \n",
    "    report_df: pd.DataFrame = pd.DataFrame(data=data)\n",
    "    report_df.to_csv(report_file, index=False) # not necessary to save it as a .csv\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dca41853",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Counting nulls..:  28%|‚ñà‚ñà‚ñä       | 24/85 [00:00<00:00, 236.20it/s]0.00s - Debugger warning: It seems that frozen modules are being used, which may\n",
      "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
      "0.00s - to python to disable frozen modules.\n",
      "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n",
      "Counting nulls..: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 85/85 [00:00<00:00, 318.51it/s]\n"
     ]
    }
   ],
   "source": [
    "count_null_instances(path=dataset_path) # count nulls found per column in each .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e9e502d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Top columns with the most nulls:\n",
      "column\n",
      "TELEFONO      4320\n",
      "DIRECTOR       116\n",
      "DIRECCION       23\n",
      "MUNICIPIO        0\n",
      "SUPERVISOR       0\n",
      "STATUS           0\n",
      "SECTOR           0\n",
      "PLAN             0\n",
      "NIVEL            0\n",
      "AREA             0\n",
      "Name: nulls, dtype: int64\n",
      "\n",
      " Files with the most nulls:\n",
      "file_name\n",
      "alta_verapaz_primaria_table9.csv        633\n",
      "alta_verapaz_preprimaria_table9.csv     376\n",
      "quiche_primaria_table9.csv              334\n",
      "peten_primaria_table9.csv               311\n",
      "jutiapa_primaria_table9.csv             277\n",
      "huehuetenango_primaria_table9.csv       258\n",
      "santa_rosa_primaria_table9.csv          225\n",
      "san_marcos_primaria_table9.csv          225\n",
      "baja_verapaz_primaria_table9.csv        166\n",
      "huehuetenango_preprimaria_table9.csv    138\n",
      "Name: nulls, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "get_null_stats(path=\"../Data_null_report/null_report.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e19e2a",
   "metadata": {},
   "source": [
    "Como podemos observar, las columnas con mayor recuento de nulos son de TELEFONO, DIRECTOR y DIRECCION. Mientras que los archivos con m√°s nulos son altaverapaz con primaria y preprimaria. \n",
    "Por lo tanto, lo que debemos hacer con esto es darles un valor a los nulos. Por ejemplo, un n√∫mero predeterminado cuando no hay, o un texto √∫nico para cuando no haya nombres o direciones, etc. <br>\n",
    "De esta manera no eliminamos ni afectamos el conjunto de datos y a√∫n logramos clasificarlos dentro de su propia categor√≠a."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61ad13a",
   "metadata": {},
   "source": [
    "### Verificaci√≥n de tipos de datos y consistencia entre archivos\n",
    "Antes de comenzar a llenar los campos vac√≠os, debemos asegurarnos que los campos que estamos llenando contengan los mismos tipos. <br>\n",
    "Es decir, que no tengamos archivos .csv que tengan tipos int64 y otros object bajo la misma columna. Ya que esto generar√° problemas a la hora<br>\n",
    "de trabajar los datos y puede generar dificultades de manejo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "838a140d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check datatypes for each column within a file\n",
    "def check_datatypes(file: str, examples: bool = False) -> tuple[defaultdict[set], defaultdict[dict]]:\n",
    "    df: pd.DataFrame = pd.read_csv(file)\n",
    "    type_map = defaultdict(set)\n",
    "    examples_map = defaultdict(dict)\n",
    "    for col in df.columns:\n",
    "        dtype = str(df[col].dtype)\n",
    "        type_map[col].add(dtype)\n",
    "        \n",
    "        if examples:\n",
    "            for val in df[col]:\n",
    "                if pd.notnull(val):\n",
    "                    val_type = str(pd.Series([val]).dtype)\n",
    "                    if val not in examples_map[col]:\n",
    "                        examples_map[col][val_type] = val\n",
    "                    break\n",
    "    return type_map, examples_map\n",
    "\n",
    "def check_datatype_differences(path: str, examples: bool = False) -> None:\n",
    "    '''\n",
    "    Checks if there is any difference between columns' datatypes across all files\n",
    "    '''\n",
    "    \n",
    "    global_type_map = defaultdict(set)\n",
    "    global_examples_map = defaultdict(dict)\n",
    "    for file in os.listdir(path):\n",
    "        if file.endswith('.csv'):\n",
    "            try: \n",
    "                f_path: str = os.path.join(path, file)\n",
    "                type_map, examples_map = check_datatypes(f_path, examples=examples)\n",
    "                for col, types in type_map.items():\n",
    "                    global_type_map[col].update(types)\n",
    "                \n",
    "                if examples:\n",
    "                    for col, example_dict in examples_map.items():\n",
    "                        for dtype, val in example_dict.items():\n",
    "                            if dtype not in global_examples_map[col]:\n",
    "                                global_examples_map[col][dtype] = val\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Unexpected error on file '{file}': {e}\")\n",
    "\n",
    "    print(f\"Columns and their types found\")\n",
    "    for col, types in global_type_map.items():\n",
    "        print(f\"- {col} : {types}\")\n",
    "        if examples and len(types) > 1:\n",
    "            for dtype, value in global_examples_map[col].items():\n",
    "                print(f\"{dtype} : {repr(value)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85f09562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns and their types found\n",
      "- CODIGO : {'object'}\n",
      "- DISTRITO : {'object'}\n",
      "- DEPARTAMENTO : {'object'}\n",
      "- MUNICIPIO : {'object'}\n",
      "- ESTABLECIMIENTO : {'object'}\n",
      "- DIRECCION : {'object'}\n",
      "- TELEFONO : {'int64', 'float64', 'object'}\n",
      "float64 : 79450881.0\n",
      "object : '77661038'\n",
      "int64 : 79414031\n",
      "- SUPERVISOR : {'object'}\n",
      "- DIRECTOR : {'object'}\n",
      "- NIVEL : {'object'}\n",
      "- SECTOR : {'object'}\n",
      "- AREA : {'object'}\n",
      "- STATUS : {'object'}\n",
      "- MODALIDAD : {'object'}\n",
      "- JORNADA : {'object'}\n",
      "- PLAN : {'object'}\n",
      "- DEPARTAMENTAL : {'object'}\n"
     ]
    }
   ],
   "source": [
    "check_datatype_differences(path=dataset_path, examples=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37199741",
   "metadata": {},
   "source": [
    "Como podemos observar arriba, tenemos que los tipos encontrados son en su mayor√≠a object, pero la columna TELEFONO posee tanto object como int64 y float 64. <br>\n",
    "Eso es un problema debido a que no podemos manejar esos 3 tipos a lo largo y ancho del conjunto de datos. Por lo que ser√° necesario indagar a fondo de por qu√© aparece\n",
    "esos tipos en el √°rea de tel√©fono. <br>\n",
    "De igual forma ser√° necesario cambiar los tipo object para que todos sean un tipo concreto y no ambiguo. Por ejemplo, darles tipo int64 si son meramente num√©ricos como los <br>\n",
    "n√∫meros telef√≥nicos o string si son direcciones o todo lo dem√°s. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "075debfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_string(value: str) -> str:\n",
    "    if pd.isnull(value):\n",
    "        return value\n",
    "    else:\n",
    "        value = str(value).strip()\n",
    "        value = value.replace('\"\"', '\"')\n",
    "        return value\n",
    "\n",
    "\n",
    "def change_dataframe_types_structure(file_path:str, columns: list[str] = None) -> pd.DataFrame:\n",
    "    '''\n",
    "    Creates a dataframe and changes all of the types to string so \n",
    "    it returns a uniform dataframe without type variance and without unnecesary quotes, punctuation marks and others\n",
    "    '''\n",
    "    df: pd.DataFrame = pd.read_csv(file_path)\n",
    "    df = df.astype({col: 'string' for col in df.select_dtypes(include=['object', 'int64', 'float64']).columns})\n",
    "\n",
    "    if columns is None:\n",
    "        columns = df.columns\n",
    "    for col in columns:\n",
    "        if col in df.columns:\n",
    "            try:\n",
    "                df[col] = df[col].map(clean_string)\n",
    "            except KeyError as k:\n",
    "                print(f\"No column found: {k}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Unexpected error on conversion: {e}\")\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f671de1",
   "metadata": {},
   "source": [
    "La funci√≥n descrita arriba la utilizaremos para cambiar los tipos en los dataframes para ejecutar los cambios en los dataframe bajo los mismos tipos. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9067793d",
   "metadata": {},
   "source": [
    "### Asignar valores a los datos inexistentes o nulos\n",
    "Ahora lo que haremos, como se dijo previamente, es asignarle valores a las celdas nulas. <br>\n",
    "Los siguientes valores predeterminados para las celdas nulas ser√°n los siguientes:<br>\n",
    "\n",
    "DIRECCION : DESCONOCIDO<br>\n",
    "TELEFONO : 00000000<br>\n",
    "DIRECTOR : DESCONOCIDO<br>\n",
    "\n",
    "Pero primero observaremos los tipos de cada columna para tener mejor definici√≥n de qu√© hay en cada una y que no exista<br>\n",
    "discrepancia entre los tipos a lo largo de las columnas y archivos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cbdc1bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill missing data with predetermined values\n",
    "missing_data_dict: dict = {\"DIRECCION\":\"DESCONOCIDO\", \"TELEFONO\": \"00000000\", \"DIRECTOR\": \"DESCONOCIDO\"} \n",
    "\n",
    "\n",
    "def fill_nulls_with_values(path:str, value_dict: dict, columns_to_clean: list[str] = None) -> list[pd.DataFrame]:\n",
    "    '''\n",
    "    Changes null instances within columns and fills them with predetermined values\n",
    "    '''\n",
    "    df_list: list[pd.DataFrame] = []\n",
    "    for file in os.listdir(path):\n",
    "        if file.endswith('.csv'):\n",
    "            f_path: str = os.path.join(path, file)\n",
    "            df: pd.DataFrame = change_dataframe_types_structure(f_path, columns=columns_to_clean) # returns a df that has all datatypes as strings and formats them correctly \n",
    "            try:\n",
    "                for col_name, default_val in value_dict.items():\n",
    "                    df[col_name] = df[col_name].fillna(default_val)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error while filling nulls with '{file}' : {e}\")\n",
    "            finally:\n",
    "                df_list.append(df)\n",
    "    \n",
    "    return df_list\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3c9d633",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CODIGO</th>\n",
       "      <th>DISTRITO</th>\n",
       "      <th>DEPARTAMENTO</th>\n",
       "      <th>MUNICIPIO</th>\n",
       "      <th>ESTABLECIMIENTO</th>\n",
       "      <th>DIRECCION</th>\n",
       "      <th>TELEFONO</th>\n",
       "      <th>SUPERVISOR</th>\n",
       "      <th>DIRECTOR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>02-01-0027-46</td>\n",
       "      <td>02-014</td>\n",
       "      <td>EL PROGRESO</td>\n",
       "      <td>GUASTATOYA</td>\n",
       "      <td>INSTITUTO TECNICO INDUSTRIAL MIXTO GUASTATOYA</td>\n",
       "      <td>BARRIO EL CALVARIO</td>\n",
       "      <td>79450881.0</td>\n",
       "      <td>CARLA MARLENY ALDANA RODAS</td>\n",
       "      <td>JOS√â ARTURO L√ìPEZ ORTIZ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>02-01-0028-46</td>\n",
       "      <td>02-014</td>\n",
       "      <td>EL PROGRESO</td>\n",
       "      <td>GUASTATOYA</td>\n",
       "      <td>COLEGIO DE CIENCIAS COMERCIALES EL PROGRESO</td>\n",
       "      <td>BARRIO EL PORVENIR</td>\n",
       "      <td>79451265.0</td>\n",
       "      <td>CARLA MARLENY ALDANA RODAS</td>\n",
       "      <td>DANIEL L√ìPEZ SOL√çS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>02-01-0031-46</td>\n",
       "      <td>02-014</td>\n",
       "      <td>EL PROGRESO</td>\n",
       "      <td>GUASTATOYA</td>\n",
       "      <td>INSTITUTO NACIONAL DE EDUCACION DIVERSIFICADA</td>\n",
       "      <td>BARRIO EL PORVENIR</td>\n",
       "      <td>54422753.0</td>\n",
       "      <td>CARLA MARLENY ALDANA RODAS</td>\n",
       "      <td>ILIANA ARACELY L√ÅZARO HERN√ÅNDEZ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>02-01-0045-46</td>\n",
       "      <td>02-014</td>\n",
       "      <td>EL PROGRESO</td>\n",
       "      <td>GUASTATOYA</td>\n",
       "      <td>INSTITUTO DE EDUCACI√ìN MEDIA POR MADUREZ</td>\n",
       "      <td>BARRIO EL PORVENIR</td>\n",
       "      <td>56719955.0</td>\n",
       "      <td>CARLA MARLENY ALDANA RODAS</td>\n",
       "      <td>NORA REBECA IBA√ëEZ MOR√ÅN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>02-01-0049-46</td>\n",
       "      <td>02-021</td>\n",
       "      <td>EL PROGRESO</td>\n",
       "      <td>GUASTATOYA</td>\n",
       "      <td>CENTRO MUNICIPAL DE EDUCACI√ìN EXTRAESCOLAR -CEEX-</td>\n",
       "      <td>COLONIA LINDA VISTA</td>\n",
       "      <td>58250827.0</td>\n",
       "      <td>MARTA ELIDA CARIAS HERNANDEZ DE GARCIA</td>\n",
       "      <td>SANDRA PAOLA MORALES GARC√çA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>02-01-0054-46</td>\n",
       "      <td>02-012</td>\n",
       "      <td>EL PROGRESO</td>\n",
       "      <td>GUASTATOYA</td>\n",
       "      <td>COLEGIO EVANGELICO TORRE FUERTE</td>\n",
       "      <td>COLONIA HICHOS, BARRIO EL PORVENIR</td>\n",
       "      <td>79451993.0</td>\n",
       "      <td>CARLOTA EUGENIA ALBUREZ AGUILAR</td>\n",
       "      <td>IRIS ORFEL√ç LOAIZA MOSCOSO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>02-01-0061-46</td>\n",
       "      <td>02-021</td>\n",
       "      <td>EL PROGRESO</td>\n",
       "      <td>GUASTATOYA</td>\n",
       "      <td>PROGRAMA NACIONAL DE EDUCACI√ìN ALTERNATIVA -PR...</td>\n",
       "      <td>BARRIO LAS JOYAS CEMENTERIO VIEJO</td>\n",
       "      <td>79637575.0</td>\n",
       "      <td>MARTA ELIDA CARIAS HERNANDEZ DE GARCIA</td>\n",
       "      <td>MARTA ELIDA CARIAS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>02-01-0062-46</td>\n",
       "      <td>02-021</td>\n",
       "      <td>EL PROGRESO</td>\n",
       "      <td>GUASTATOYA</td>\n",
       "      <td>CENTRO DE EDUCACI√ìN EXTRAESCOLAR -CEEX- DEPART...</td>\n",
       "      <td>BARRIO LAS JOYAS, CEMENTERIO VIEJO</td>\n",
       "      <td>37962236.0</td>\n",
       "      <td>MARTA ELIDA CARIAS HERNANDEZ DE GARCIA</td>\n",
       "      <td>HELLEN CELESTE MARROQUIN CORADO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>02-01-0066-46</td>\n",
       "      <td>02-012</td>\n",
       "      <td>EL PROGRESO</td>\n",
       "      <td>GUASTATOYA</td>\n",
       "      <td>COLEGIO MIXTO INTEGRAL EL PROGRESO</td>\n",
       "      <td>COLONIA HICHOS, BARRIO EL PORVENIR</td>\n",
       "      <td>47406758.0</td>\n",
       "      <td>CARLOTA EUGENIA ALBUREZ AGUILAR</td>\n",
       "      <td>DELFINA MARISOL PAZOS RAMOS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          CODIGO DISTRITO DEPARTAMENTO   MUNICIPIO  \\\n",
       "0  02-01-0027-46   02-014  EL PROGRESO  GUASTATOYA   \n",
       "1  02-01-0028-46   02-014  EL PROGRESO  GUASTATOYA   \n",
       "2  02-01-0031-46   02-014  EL PROGRESO  GUASTATOYA   \n",
       "3  02-01-0045-46   02-014  EL PROGRESO  GUASTATOYA   \n",
       "4  02-01-0049-46   02-021  EL PROGRESO  GUASTATOYA   \n",
       "5  02-01-0054-46   02-012  EL PROGRESO  GUASTATOYA   \n",
       "6  02-01-0061-46   02-021  EL PROGRESO  GUASTATOYA   \n",
       "7  02-01-0062-46   02-021  EL PROGRESO  GUASTATOYA   \n",
       "8  02-01-0066-46   02-012  EL PROGRESO  GUASTATOYA   \n",
       "\n",
       "                                     ESTABLECIMIENTO  \\\n",
       "0      INSTITUTO TECNICO INDUSTRIAL MIXTO GUASTATOYA   \n",
       "1        COLEGIO DE CIENCIAS COMERCIALES EL PROGRESO   \n",
       "2      INSTITUTO NACIONAL DE EDUCACION DIVERSIFICADA   \n",
       "3           INSTITUTO DE EDUCACI√ìN MEDIA POR MADUREZ   \n",
       "4  CENTRO MUNICIPAL DE EDUCACI√ìN EXTRAESCOLAR -CEEX-   \n",
       "5                    COLEGIO EVANGELICO TORRE FUERTE   \n",
       "6  PROGRAMA NACIONAL DE EDUCACI√ìN ALTERNATIVA -PR...   \n",
       "7  CENTRO DE EDUCACI√ìN EXTRAESCOLAR -CEEX- DEPART...   \n",
       "8                 COLEGIO MIXTO INTEGRAL EL PROGRESO   \n",
       "\n",
       "                            DIRECCION    TELEFONO  \\\n",
       "0                  BARRIO EL CALVARIO  79450881.0   \n",
       "1                  BARRIO EL PORVENIR  79451265.0   \n",
       "2                  BARRIO EL PORVENIR  54422753.0   \n",
       "3                  BARRIO EL PORVENIR  56719955.0   \n",
       "4                 COLONIA LINDA VISTA  58250827.0   \n",
       "5  COLONIA HICHOS, BARRIO EL PORVENIR  79451993.0   \n",
       "6   BARRIO LAS JOYAS CEMENTERIO VIEJO  79637575.0   \n",
       "7  BARRIO LAS JOYAS, CEMENTERIO VIEJO  37962236.0   \n",
       "8  COLONIA HICHOS, BARRIO EL PORVENIR  47406758.0   \n",
       "\n",
       "                               SUPERVISOR                         DIRECTOR  \n",
       "0              CARLA MARLENY ALDANA RODAS          JOS√â ARTURO L√ìPEZ ORTIZ  \n",
       "1              CARLA MARLENY ALDANA RODAS               DANIEL L√ìPEZ SOL√çS  \n",
       "2              CARLA MARLENY ALDANA RODAS  ILIANA ARACELY L√ÅZARO HERN√ÅNDEZ  \n",
       "3              CARLA MARLENY ALDANA RODAS         NORA REBECA IBA√ëEZ MOR√ÅN  \n",
       "4  MARTA ELIDA CARIAS HERNANDEZ DE GARCIA      SANDRA PAOLA MORALES GARC√çA  \n",
       "5         CARLOTA EUGENIA ALBUREZ AGUILAR       IRIS ORFEL√ç LOAIZA MOSCOSO  \n",
       "6  MARTA ELIDA CARIAS HERNANDEZ DE GARCIA               MARTA ELIDA CARIAS  \n",
       "7  MARTA ELIDA CARIAS HERNANDEZ DE GARCIA  HELLEN CELESTE MARROQUIN CORADO  \n",
       "8         CARLOTA EUGENIA ALBUREZ AGUILAR      DELFINA MARISOL PAZOS RAMOS  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_list: list[pd.DataFrame] = fill_nulls_with_values(path=dataset_path, value_dict=missing_data_dict, columns_to_clean=['ESTABLECIMIENTO'])\n",
    "df_list[0].iloc[:9, :9]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f14f51",
   "metadata": {},
   "source": [
    "### Transformar las palabras a forma est√°ndar; quitar tildes y di√©resis\n",
    "\n",
    "Una vez que hemos llenado los valores faltantes, es crucial estandarizar el formato del texto en todo el conjunto de datos. Esto es especialmente importante para garantizar consistencia y evitar problemas en an√°lisis posteriores donde palabras iguales pero escritas de forma diferente sean tratadas como distintas.\n",
    "\n",
    "**Objetivos de esta estandarizaci√≥n:**\n",
    "\n",
    "1. **Eliminar acentos y caracteres especiales**: Normalizar palabras como \"Jos√©\" ‚Üí \"Jose\", \"Quich√©\" ‚Üí \"Quiche\"\n",
    "2. **Unificar formatos por tipo de campo**:\n",
    "   - **Establecimientos**: TODO EN MAY√öSCULAS para consistencia\n",
    "   - **Nombres propios** (Director/Supervisor): Formato T√≠tulo (Primera Letra May√∫scula)\n",
    "   - **Direcciones**: Expandir abreviaciones (Ave ‚Üí Avenida, Km ‚Üí Kilometro)\n",
    "   - **Tel√©fonos**: Formato uniforme de 8 d√≠gitos sin separadores\n",
    "3. **Limpiar espacios y puntuaci√≥n**: Eliminar espacios m√∫ltiples y comillas redundantes\n",
    "\n",
    "**Transformaciones espec√≠ficas aplicadas:**\n",
    "\n",
    "- **Normalizaci√≥n Unicode**: Separar caracteres base de sus acentos y eliminar los acentos\n",
    "- **Estandarizaci√≥n de direcciones**: \"5a. ave 1-23 zona 4\" ‚Üí \"5A AVENIDA 1-23 ZONA 4\"\n",
    "- **Formato de tel√©fonos**: \"7794-5104\" ‚Üí \"77945104\", \"79529782.0\" ‚Üí \"79529782\"\n",
    "- **Nombres propios**: \"JOS√â MAR√çA HERN√ÅNDEZ\" ‚Üí \"Jose Maria Hernandez\"\n",
    "- **Establecimientos**: \"Colegio San Jos√©\" ‚Üí \"COLEGIO SAN JOSE\"\n",
    "\n",
    "Esta estandarizaci√≥n es fundamental para:\n",
    "- Evitar duplicados por diferencias tipogr√°ficas\n",
    "- Facilitar b√∫squedas y filtros posteriores  \n",
    "- Garantizar consistencia en el conjunto de datos final\n",
    "- Preparar los datos para an√°lisis automatizados\n",
    "\n",
    "**Proceso:** Se aplicar√° la estandarizaci√≥n a todos los DataFrames que ya contienen los valores nulos rellenados, manteniendo la integridad de los datos mientras se mejora su formato y consistencia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c8488e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aplicando estandarizaci√≥n de texto...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Estandarizando DataFrames: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 85/85 [00:01<00:00, 44.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Estandarizaci√≥n completada para 85 DataFrames\n",
      "\n",
      "üìä RESUMEN:\n",
      "‚Ä¢ Total de archivos estandarizados: 85\n",
      "‚Ä¢ Total de registros procesados: 38057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def remove_accents_and_special_chars(text: str) -> str:\n",
    "    \"\"\"Remueve tildes, di√©resis y normaliza caracteres especiales\"\"\"\n",
    "    if pd.isnull(text):\n",
    "        return text\n",
    "    \n",
    "    text = str(text).strip()\n",
    "    # Normalizar unicode y remover acentos\n",
    "    text = unicodedata.normalize('NFD', text)\n",
    "    text = ''.join(char for char in text if unicodedata.category(char) != 'Mn')\n",
    "    return text\n",
    "\n",
    "def standardize_phone_number(phone: str) -> str:\n",
    "    \"\"\"Estandariza n√∫meros telef√≥nicos a formato consistente (8 d√≠gitos)\"\"\"\n",
    "    if pd.isnull(phone) or phone == \"00000000\":\n",
    "        return \"00000000\"\n",
    "    \n",
    "    phone_str = str(phone).strip()\n",
    "    digits_only = re.sub(r'\\D', '', phone_str)\n",
    "    \n",
    "    if len(digits_only) == 8:\n",
    "        return digits_only\n",
    "    elif len(digits_only) > 8:\n",
    "        return digits_only[-8:]\n",
    "    elif len(digits_only) < 8 and len(digits_only) > 0:\n",
    "        return digits_only.zfill(8)\n",
    "    else:\n",
    "        return \"00000000\"\n",
    "\n",
    "def standardize_address(address: str) -> str:\n",
    "    \"\"\"Estandariza formato de direcciones\"\"\"\n",
    "    if pd.isnull(address) or address == \"DESCONOCIDO\":\n",
    "        return \"DESCONOCIDO\"\n",
    "    \n",
    "    address = str(address).strip().upper()\n",
    "    \n",
    "    # Estandarizar abreviaciones comunes\n",
    "    replacements = {\n",
    "        r'\\bAV\\b\\.?': 'AVENIDA',\n",
    "        r'\\bAVE\\b\\.?': 'AVENIDA', \n",
    "        r'\\bCALL\\b\\.?': 'CALLE',\n",
    "        r'\\bKM\\b\\.?': 'KILOMETRO',\n",
    "        r'\\bZON\\b\\.?': 'ZONA',\n",
    "        r'\\bALD\\b\\.?': 'ALDEA'\n",
    "    }\n",
    "    \n",
    "    for pattern, replacement in replacements.items():\n",
    "        address = re.sub(pattern, replacement, address)\n",
    "    \n",
    "    address = re.sub(r'\\s+', ' ', address)\n",
    "    return address.strip()\n",
    "\n",
    "def comprehensive_text_cleaning(text: str, field_type: str = 'general') -> str:\n",
    "    \"\"\"Aplica limpieza comprehensiva seg√∫n el tipo de campo\"\"\"\n",
    "    if pd.isnull(text):\n",
    "        return text\n",
    "    \n",
    "    # Limpieza b√°sica\n",
    "    text = str(text).strip()\n",
    "    text = text.replace('\"\"', '\"').replace(\"''\", \"'\")\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = remove_accents_and_special_chars(text)\n",
    "    \n",
    "    # Aplicar estandarizaci√≥n espec√≠fica\n",
    "    if field_type == 'phone':\n",
    "        return standardize_phone_number(text)\n",
    "    elif field_type == 'address':\n",
    "        return standardize_address(text)\n",
    "    elif field_type == 'name':\n",
    "        # Para nombres: Primera letra may√∫scula, resto min√∫scula\n",
    "        return ' '.join(word.capitalize() for word in text.split())\n",
    "    elif field_type == 'establishment':\n",
    "        return text.upper()\n",
    "    else:\n",
    "        return text.upper()\n",
    "\n",
    "def apply_standardization_to_dataframes(df_list: list[pd.DataFrame]) -> list[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Aplica estandarizaci√≥n de texto a una lista de DataFrames\n",
    "    \"\"\"\n",
    "    standardized_dfs = []\n",
    "    \n",
    "    # Definir qu√© tipo de limpieza aplicar a cada columna\n",
    "    field_mappings = {\n",
    "        'TELEFONO': 'phone',\n",
    "        'DIRECCION': 'address', \n",
    "        'DIRECTOR': 'name',\n",
    "        'SUPERVISOR': 'name',\n",
    "        'ESTABLECIMIENTO': 'establishment',\n",
    "        'DEPARTAMENTO': 'general',\n",
    "        'MUNICIPIO': 'general',\n",
    "        'DISTRITO': 'general',\n",
    "        'NIVEL': 'general',\n",
    "        'SECTOR': 'general',\n",
    "        'AREA': 'general',\n",
    "        'STATUS': 'general',\n",
    "        'MODALIDAD': 'general',\n",
    "        'JORNADA': 'general',\n",
    "        'PLAN': 'general',\n",
    "        'DEPARTAMENTAL': 'general',\n",
    "        'CODIGO': 'general'\n",
    "    }\n",
    "    \n",
    "    print(\"Aplicando estandarizaci√≥n de texto...\")\n",
    "    \n",
    "    for i, df in enumerate(tqdm(df_list, desc=\"Estandarizando DataFrames\")):\n",
    "        df_clean = df.copy()\n",
    "        \n",
    "        # Aplicar limpieza espec√≠fica a cada columna\n",
    "        for column, field_type in field_mappings.items():\n",
    "            if column in df_clean.columns:\n",
    "                df_clean[column] = df_clean[column].apply(\n",
    "                    lambda x: comprehensive_text_cleaning(x, field_type)\n",
    "                )\n",
    "        \n",
    "        standardized_dfs.append(df_clean)\n",
    "    \n",
    "    print(f\"‚úÖ Estandarizaci√≥n completada para {len(standardized_dfs)} DataFrames\")\n",
    "    return standardized_dfs\n",
    "\n",
    "# Aplicar estandarizaci√≥n a los DataFrames que ya tienes\n",
    "df_list_standardized = apply_standardization_to_dataframes(df_list)\n",
    "\n",
    "print(f\"\\nüìä RESUMEN:\")\n",
    "print(f\"‚Ä¢ Total de archivos estandarizados: {len(df_list_standardized)}\")\n",
    "print(f\"‚Ä¢ Total de registros procesados: {sum(len(df) for df in df_list_standardized)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b3f199",
   "metadata": {},
   "source": [
    "### Concatenar todo a un mismo .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "177b6c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86699034",
   "metadata": {},
   "source": [
    "### Estandarizar el csv conjunto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2aec070d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
