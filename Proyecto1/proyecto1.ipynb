{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0894d108",
   "metadata": {},
   "source": [
    "# Primera entrega\n",
    "Luis Montenegro - 21699<br>\n",
    "Javier Prado - 21486<br>\n",
    "Bryan España<br>\n",
    "Ángel Herrarte<br>\n",
    "\n",
    "\n",
    "## Limpieza de datos\n",
    "En esta entrega se hará una recolección y limpieza de datos solamente. No habrá modelado ni análisis. \n",
    "<br>Solamente velar por la integridad, coherencia y cohesión del conjunto de datos.facilidad de manejo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ff6290c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa43cd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import unicodedata\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a3fcc9",
   "metadata": {},
   "source": [
    "### Conversión de .xls a .csv\n",
    "\n",
    "Debido a que los archivos crudos descargados desde la página del Mineduc vienen en formato .xls formateado como .html tenemos que hacer cambio de eso.<br>\n",
    "Esto por múltiples razones:\n",
    "- Mejorar la estructura de los datos.\n",
    "- Preservar solamente la información requerida (los .xls almaceban más páginas e información irrelevante)\n",
    "- Mayor falicidad de usar y obtener los datos si están en formato delimitado por comas.\n",
    "- Tener datos más ligeros ya que no acarreamos con mucha información innecesaria. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "456d91fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_html_to_csv(input_path: str, output_path: str=None, tables_to_convert: list[int] = [9]) -> None:\n",
    "    '''\n",
    "    Turns .html files into .csv files\n",
    "\n",
    "    Params:\n",
    "        input_path: where the .html files are stored\n",
    "        output_path: where the .csv will be stored\n",
    "    Returns:\n",
    "        out: None\n",
    "    '''\n",
    "    if output_path is None:\n",
    "        output_path = input_path \n",
    "    else:\n",
    "        os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "    for filename in tqdm(os.listdir(input_path), desc='Converting Files'):\n",
    "        if filename.endswith('.xls'):\n",
    "            html_path:str = os.path.join(input_path, filename)\n",
    "            base_name:str = os.path.splitext(filename)[0]\n",
    "            # try to read the excel and copy it into a .csv file\n",
    "            try: \n",
    "                tables:list[pd.DataFrame] = pd.read_html(html_path)\n",
    "                # if no tables, skip\n",
    "                if not tables:\n",
    "                    print(f\"No tables found in '{filename}'\")\n",
    "                    continue\n",
    "\n",
    "                # convert each of the tables selected to .csv\n",
    "                for i, df in enumerate(tables):\n",
    "                    if i in tables_to_convert:\n",
    "                        csv_filename = f\"{base_name}_table{i}.csv\"\n",
    "                        csv_path = os.path.join(output_path, csv_filename)\n",
    "                        df.to_csv(csv_path, index=False)\n",
    "                        print(f\"Converted '{filename}' to '{csv_filename}'\") # print all of the transformations of the .xls to its respective .csv files\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file '{filename}' : {e}\")\n",
    "\n",
    "def remove_files(path: str, extension: str) -> None:\n",
    "    '''\n",
    "    Removes all files that have a certain extension\n",
    "    Input:\n",
    "        path: folder path where .xls to be removed are stored\n",
    "    '''\n",
    "    file_list: list[str] = os.listdir(path)\n",
    "    if not file_list:\n",
    "        print(f\"No files removed since there was none found: '{path}'\")\n",
    "        return\n",
    "\n",
    "    for filename in file_list:\n",
    "    \n",
    "        if filename.endswith(extension):\n",
    "            try:\n",
    "                xls_path = os.path.join(path, filename)\n",
    "                os.remove(xls_path)\n",
    "            except Exception as e:\n",
    "                print(f\"Error removing file '{filename}' : {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f249f5f7",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../Dataset_raw'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# start by turning .xls files to .csv files\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mtransform_html_to_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_path\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m../Dataset_raw\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m../Dataset_cleaned\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtables_to_convert\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m9\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 16\u001b[39m, in \u001b[36mtransform_html_to_csv\u001b[39m\u001b[34m(input_path, output_path, tables_to_convert)\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     14\u001b[39m     os.makedirs(output_path, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_path\u001b[49m\u001b[43m)\u001b[49m, desc=\u001b[33m'\u001b[39m\u001b[33mConverting Files\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m     17\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m filename.endswith(\u001b[33m'\u001b[39m\u001b[33m.xls\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m     18\u001b[39m         html_path:\u001b[38;5;28mstr\u001b[39m = os.path.join(input_path, filename)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '../Dataset_raw'"
     ]
    }
   ],
   "source": [
    "# start by turning .xls files to .csv files\n",
    "transform_html_to_csv(input_path=\"../Dataset_raw\", output_path=\"../Dataset_cleaned\", tables_to_convert=[9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81a4e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove .xls files if necessary\n",
    "remove_files(path=\"../Dataset_raw\", extension='.xls')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65016283",
   "metadata": {},
   "source": [
    "### Estandarizar la estructura de los .csv\n",
    "Es importante que los .csv con los que estamos trabajando tengan una misma estructura. Por lo tanto, debemos asegurarnos que todos sean iguales. <br>\n",
    "Esto lo hacemos con la finalidad de evitar errores al limpiar los datos más adelante o al concatenarlos al final. \n",
    "\n",
    "Queremos que todos los .csv tengan 17 títulos de columnas en orden tipo:<br>\n",
    "CODIGO,DISTRITO,DEPARTAMENTO,MUNICIPIO,ESTABLECIMIENTO,DIRECCION,TELEFONO,SUPERVISOR,DIRECTOR,NIVEL,SECTOR,AREA,STATUS,MODALIDAD,JORNADA,PLAN,DEPARTAMENTAL<br>\n",
    "\n",
    "Otra cosa a considerar es eliminar la primera fila y reajustar índices. Debido a que la primera fila son puros números en vez de los nombres de las columnas. Al igual que debemos\n",
    "eliminar la última fila que es todos nulos. \n",
    "\n",
    "Para los que tenían error de formato o formato diferente solamente copiamos los datos en un .csv ya que usualmente eso se debe a que solamente había 1 establecimiento encontrado.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3558893f",
   "metadata": {},
   "outputs": [],
   "source": [
    "COLUMNS = [\"CODIGO\",\"DISTRITO\",\"DEPARTAMENTO\",\"MUNICIPIO\",\"ESTABLECIMIENTO\",\"DIRECCION\",\"TELEFONO\",\"SUPERVISOR\",\"DIRECTOR\",\"NIVEL\",\"SECTOR\",\"AREA\",\"STATUS\",\"MODALIDAD\",\"JORNADA\",\"PLAN\",\"DEPARTAMENTAL\"]\n",
    "\n",
    "def clean_headers_and_trailers(path: str) -> None:\n",
    "    '''\n",
    "    Removes first row and reindexes the data within.\n",
    "    Also removes the last row that contains only nulls\n",
    "    '''\n",
    "    for file in tqdm(os.listdir(path), desc=\"Converting files\"):\n",
    "        if file.endswith('.csv'):\n",
    "            try:\n",
    "                f_path = os.path.join(path, file) # path to the file\n",
    "                df: pd.DataFrame = pd.read_csv(f_path)\n",
    "                \n",
    "                # drop first row and reindex if the first row are digits and not columns\n",
    "                if all(col.strip().isdigit() for col in df.columns):\n",
    "                    df.columns = df.iloc[0]\n",
    "                    df = df[1:].reset_index(drop=True)\n",
    "\n",
    "                # remove last row if only nulls are found\n",
    "                if df.tail(1).isnull().all(axis=1).iloc[0]:\n",
    "                    df = df.iloc[:-1]\n",
    "                    print(f\"Removed last row: {file}\")\n",
    "                \n",
    "                df.to_csv(f_path, index=False)  \n",
    "                \n",
    "                print(f\"Finished: {file}\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing '{file}': {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99e3f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean headers and trailers\n",
    "# clean_headers_and_trailers(path = \"../Dataset_cleaned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7e543e",
   "metadata": {},
   "source": [
    "### Plan para limpieza de datos\n",
    "Posteriormente a la transformación de formato y filtración a solo datos crudos de nuestro interés, procederemos a limpiar los datos en sí. <br>\n",
    "1. Verificaremos si existe datos NA o Null en los archivos. Llenaremos los datos faltantes con datos artificiales ya que no podemos eliminar ningún registro <br>\n",
    "debido a que si eliminamos alguno, sería una sede faltante. Cosa que afectaría de gran manera el conjunto de datos.\n",
    "2. Homogenizaremos la información. Es decir, que todos los archivos tengan el mismo formato i.e. todos los nombres estén escritos igual, los apellidos que tienen tildes se escriban igual en cada archivo,\n",
    "que las palabras vengan o solo en mayúsculas, solo en minúsculas, etc.\n",
    "3. Identificaremos las columnas que más trabajo de reajuste necesiten.\n",
    "- Nombres (Supervisor, Director, Establecimiento, Sector): debido a que puede que exista apellidos iguales pero escritos distinto y eso genere problemas a la hora de verlos. Digamos, puede que Hernández aparezca con tilde en unos registros y en otros no. También es bueno verificar que estén escritos todos o en mayúsculas o en minúsculas (o todos iguales), ya que digamos, si tenemos Santa rosa en un registro y Santa Rosa en otro, a la hora de hacer un encoding estos resultarán con dos valores distintos. \n",
    "- Dirección: Asegurarnos que las direcciones lleven una estructura similar.\n",
    "- Códigos: Verificar que los códigos de los registros sean únicos y evitar tener repetidos.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6208bf",
   "metadata": {},
   "source": [
    "El conjunto de datos corresponde a los establecimientos educativos de Guatemala que llegan hasta el nivel diversificado. Está organizado en 85 archivos CSV, uno por cada departamento y municipio del país.\n",
    "\n",
    "Cada archivo contiene información detallada de cada establecimiento, como el nombre del centro, su ubicación, datos de contacto, modalidad de enseñanza y otros datos administrativos.\n",
    "\n",
    "\n",
    "Los archivos csv estan clasificados por departamentos de Guatemala. Las variables que contiene esta dataset son las siguientes sumando un total de 17 variables a analizar.\n",
    "\n",
    "- CODIGO: código único del establecimiento\n",
    "\n",
    "- DISTRITO: distrito educativo\n",
    "\n",
    "- DEPARTAMENTO: nombre del departamento\n",
    "\n",
    "- MUNICIPIO: municipio donde se ubica\n",
    "\n",
    "- ESTABLECIMIENTO: nombre del centro educativo\n",
    "\n",
    "- DIRECCION: dirección física\n",
    "\n",
    "- TELEFONO: número de contacto\n",
    "\n",
    "- SUPERVISOR: nombre del supervisor\n",
    "\n",
    "- DIRECTOR: nombre del director\n",
    "\n",
    "- NIVEL: nivel educativo (ej. Básico, Diversificado)\n",
    "\n",
    "- SECTOR: sector oficial o privado\n",
    "\n",
    "- AREA: área urbana o rural\n",
    "\n",
    "- STATUS: estado del centro (ej. Abierta)\n",
    "\n",
    "- MODALIDAD: modalidad lingüística (ej. Monolingüe, Bilingüe)\n",
    "\n",
    "- JORNADA: jornada de estudio (ej. Matutina, Vespertina)\n",
    "\n",
    "- PLAN: plan educativo (ej. Diario, Fin de semana)\n",
    "\n",
    "- DEPARTAMENTAL: nombre del departamento de adscripción"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0657006d",
   "metadata": {},
   "source": [
    "### Encontrar Null o NA en cualesquiera archivos\n",
    "Comenzaremos viendo si existe cualesquiera archivos con datos faltantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37e5db5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path: str = \"../Dataset_cleaned\" # define folder path as a variable for easier handling\n",
    "report_path: str = \"../Data_null_report\"\n",
    "\n",
    "def get_null_stats(path:str) -> None:\n",
    "        '''\n",
    "        Prints the columns with most null counts and the files with most nulls\n",
    "        '''\n",
    "        df: pd.DataFrame = pd.read_csv(path)\n",
    "        print(\"\\n Top columns with the most nulls:\")\n",
    "        top_columns = df.groupby('column')['nulls'].sum().sort_values(ascending=False).head(10)\n",
    "        print(top_columns)\n",
    "\n",
    "        print(\"\\n Files with the most nulls:\")\n",
    "        top_files = df.groupby('file_name')['nulls'].sum().sort_values(ascending=False).head(10)\n",
    "        print(top_files)\n",
    "\n",
    "def count_null_instances(path:str, report_name:str = \"null_report.csv\") -> None:\n",
    "    '''\n",
    "    Counts how many missing instances are found per column per .csv and saves it into a .csv\n",
    "    '''\n",
    "    files: list[str] = os.listdir(path=path)\n",
    "    report_file: str = os.path.join(report_path, report_name)\n",
    "    os.makedirs(name=report_path, exist_ok=True) # make a new directory to save this information\n",
    "    data:list[dict] = []\n",
    "    for file in tqdm(files, desc=\"Counting nulls..\"):\n",
    "        file_path:str = os.path.join(path, file)\n",
    "        try:\n",
    "            df: pd.DataFrame = pd.read_csv(file_path) \n",
    "            null_counts = df.isnull().sum() # register nulls found in the df per column\n",
    "            for col, count in null_counts.items():\n",
    "                data.append({\n",
    "                    \"file_name\": file,\n",
    "                    \"column\": col,\n",
    "                    \"nulls\": count\n",
    "                })\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file} : {e}\")\n",
    "    \n",
    "    report_df: pd.DataFrame = pd.DataFrame(data=data)\n",
    "    report_df.to_csv(report_file, index=False) # not necessary to save it as a .csv\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dca41853",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Counting nulls..:  28%|██▊       | 24/85 [00:00<00:00, 236.20it/s]0.00s - Debugger warning: It seems that frozen modules are being used, which may\n",
      "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
      "0.00s - to python to disable frozen modules.\n",
      "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n",
      "Counting nulls..: 100%|██████████| 85/85 [00:00<00:00, 318.51it/s]\n"
     ]
    }
   ],
   "source": [
    "count_null_instances(path=dataset_path) # count nulls found per column in each .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e9e502d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Top columns with the most nulls:\n",
      "column\n",
      "TELEFONO      4320\n",
      "DIRECTOR       116\n",
      "DIRECCION       23\n",
      "MUNICIPIO        0\n",
      "SUPERVISOR       0\n",
      "STATUS           0\n",
      "SECTOR           0\n",
      "PLAN             0\n",
      "NIVEL            0\n",
      "AREA             0\n",
      "Name: nulls, dtype: int64\n",
      "\n",
      " Files with the most nulls:\n",
      "file_name\n",
      "alta_verapaz_primaria_table9.csv        633\n",
      "alta_verapaz_preprimaria_table9.csv     376\n",
      "quiche_primaria_table9.csv              334\n",
      "peten_primaria_table9.csv               311\n",
      "jutiapa_primaria_table9.csv             277\n",
      "huehuetenango_primaria_table9.csv       258\n",
      "santa_rosa_primaria_table9.csv          225\n",
      "san_marcos_primaria_table9.csv          225\n",
      "baja_verapaz_primaria_table9.csv        166\n",
      "huehuetenango_preprimaria_table9.csv    138\n",
      "Name: nulls, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "get_null_stats(path=\"../Data_null_report/null_report.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e19e2a",
   "metadata": {},
   "source": [
    "Como podemos observar, las columnas con mayor recuento de nulos son de TELEFONO, DIRECTOR y DIRECCION. Mientras que los archivos con más nulos son altaverapaz con primaria y preprimaria. \n",
    "Por lo tanto, lo que debemos hacer con esto es darles un valor a los nulos. Por ejemplo, un número predeterminado cuando no hay, o un texto único para cuando no haya nombres o direciones, etc. <br>\n",
    "De esta manera no eliminamos ni afectamos el conjunto de datos y aún logramos clasificarlos dentro de su propia categoría."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61ad13a",
   "metadata": {},
   "source": [
    "### Verificación de tipos de datos y consistencia entre archivos\n",
    "Antes de comenzar a llenar los campos vacíos, debemos asegurarnos que los campos que estamos llenando contengan los mismos tipos. <br>\n",
    "Es decir, que no tengamos archivos .csv que tengan tipos int64 y otros object bajo la misma columna. Ya que esto generará problemas a la hora<br>\n",
    "de trabajar los datos y puede generar dificultades de manejo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "838a140d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check datatypes for each column within a file\n",
    "def check_datatypes(file: str, examples: bool = False) -> tuple[defaultdict[set], defaultdict[dict]]:\n",
    "    df: pd.DataFrame = pd.read_csv(file)\n",
    "    type_map = defaultdict(set)\n",
    "    examples_map = defaultdict(dict)\n",
    "    for col in df.columns:\n",
    "        dtype = str(df[col].dtype)\n",
    "        type_map[col].add(dtype)\n",
    "        \n",
    "        if examples:\n",
    "            for val in df[col]:\n",
    "                if pd.notnull(val):\n",
    "                    val_type = str(pd.Series([val]).dtype)\n",
    "                    if val not in examples_map[col]:\n",
    "                        examples_map[col][val_type] = val\n",
    "                    break\n",
    "    return type_map, examples_map\n",
    "\n",
    "def check_datatype_differences(path: str, examples: bool = False) -> None:\n",
    "    '''\n",
    "    Checks if there is any difference between columns' datatypes across all files\n",
    "    '''\n",
    "    \n",
    "    global_type_map = defaultdict(set)\n",
    "    global_examples_map = defaultdict(dict)\n",
    "    for file in os.listdir(path):\n",
    "        if file.endswith('.csv'):\n",
    "            try: \n",
    "                f_path: str = os.path.join(path, file)\n",
    "                type_map, examples_map = check_datatypes(f_path, examples=examples)\n",
    "                for col, types in type_map.items():\n",
    "                    global_type_map[col].update(types)\n",
    "                \n",
    "                if examples:\n",
    "                    for col, example_dict in examples_map.items():\n",
    "                        for dtype, val in example_dict.items():\n",
    "                            if dtype not in global_examples_map[col]:\n",
    "                                global_examples_map[col][dtype] = val\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Unexpected error on file '{file}': {e}\")\n",
    "\n",
    "    print(f\"Columns and their types found\")\n",
    "    for col, types in global_type_map.items():\n",
    "        print(f\"- {col} : {types}\")\n",
    "        if examples and len(types) > 1:\n",
    "            for dtype, value in global_examples_map[col].items():\n",
    "                print(f\"{dtype} : {repr(value)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85f09562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns and their types found\n",
      "- CODIGO : {'object'}\n",
      "- DISTRITO : {'object'}\n",
      "- DEPARTAMENTO : {'object'}\n",
      "- MUNICIPIO : {'object'}\n",
      "- ESTABLECIMIENTO : {'object'}\n",
      "- DIRECCION : {'object'}\n",
      "- TELEFONO : {'int64', 'float64', 'object'}\n",
      "float64 : 79450881.0\n",
      "object : '77661038'\n",
      "int64 : 79414031\n",
      "- SUPERVISOR : {'object'}\n",
      "- DIRECTOR : {'object'}\n",
      "- NIVEL : {'object'}\n",
      "- SECTOR : {'object'}\n",
      "- AREA : {'object'}\n",
      "- STATUS : {'object'}\n",
      "- MODALIDAD : {'object'}\n",
      "- JORNADA : {'object'}\n",
      "- PLAN : {'object'}\n",
      "- DEPARTAMENTAL : {'object'}\n"
     ]
    }
   ],
   "source": [
    "check_datatype_differences(path=dataset_path, examples=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37199741",
   "metadata": {},
   "source": [
    "Como podemos observar arriba, tenemos que los tipos encontrados son en su mayoría object, pero la columna TELEFONO posee tanto object como int64 y float 64. <br>\n",
    "Eso es un problema debido a que no podemos manejar esos 3 tipos a lo largo y ancho del conjunto de datos. Por lo que será necesario indagar a fondo de por qué aparece\n",
    "esos tipos en el área de teléfono. <br>\n",
    "De igual forma será necesario cambiar los tipo object para que todos sean un tipo concreto y no ambiguo. Por ejemplo, darles tipo int64 si son meramente numéricos como los <br>\n",
    "números telefónicos o string si son direcciones o todo lo demás. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "075debfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_string(value: str) -> str:\n",
    "    if pd.isnull(value):\n",
    "        return value\n",
    "    else:\n",
    "        value = str(value).strip()\n",
    "        value = value.replace('\"\"', '\"')\n",
    "        return value\n",
    "\n",
    "\n",
    "def change_dataframe_types_structure(file_path:str, columns: list[str] = None) -> pd.DataFrame:\n",
    "    '''\n",
    "    Creates a dataframe and changes all of the types to string so \n",
    "    it returns a uniform dataframe without type variance and without unnecesary quotes, punctuation marks and others\n",
    "    '''\n",
    "    df: pd.DataFrame = pd.read_csv(file_path)\n",
    "    df = df.astype({col: 'string' for col in df.select_dtypes(include=['object', 'int64', 'float64']).columns})\n",
    "\n",
    "    if columns is None:\n",
    "        columns = df.columns\n",
    "    for col in columns:\n",
    "        if col in df.columns:\n",
    "            try:\n",
    "                df[col] = df[col].map(clean_string)\n",
    "            except KeyError as k:\n",
    "                print(f\"No column found: {k}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Unexpected error on conversion: {e}\")\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f671de1",
   "metadata": {},
   "source": [
    "La función descrita arriba la utilizaremos para cambiar los tipos en los dataframes para ejecutar los cambios en los dataframe bajo los mismos tipos. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9067793d",
   "metadata": {},
   "source": [
    "### Asignar valores a los datos inexistentes o nulos\n",
    "Ahora lo que haremos, como se dijo previamente, es asignarle valores a las celdas nulas. <br>\n",
    "Los siguientes valores predeterminados para las celdas nulas serán los siguientes:<br>\n",
    "\n",
    "DIRECCION : DESCONOCIDO<br>\n",
    "TELEFONO : 00000000<br>\n",
    "DIRECTOR : DESCONOCIDO<br>\n",
    "\n",
    "Pero primero observaremos los tipos de cada columna para tener mejor definición de qué hay en cada una y que no exista<br>\n",
    "discrepancia entre los tipos a lo largo de las columnas y archivos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cbdc1bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill missing data with predetermined values\n",
    "missing_data_dict: dict = {\"DIRECCION\":\"DESCONOCIDO\", \"TELEFONO\": \"00000000\", \"DIRECTOR\": \"DESCONOCIDO\"} \n",
    "\n",
    "\n",
    "def fill_nulls_with_values(path:str, value_dict: dict, columns_to_clean: list[str] = None) -> list[pd.DataFrame]:\n",
    "    '''\n",
    "    Changes null instances within columns and fills them with predetermined values\n",
    "    '''\n",
    "    df_list: list[pd.DataFrame] = []\n",
    "    for file in os.listdir(path):\n",
    "        if file.endswith('.csv'):\n",
    "            f_path: str = os.path.join(path, file)\n",
    "            df: pd.DataFrame = change_dataframe_types_structure(f_path, columns=columns_to_clean) # returns a df that has all datatypes as strings and formats them correctly \n",
    "            try:\n",
    "                for col_name, default_val in value_dict.items():\n",
    "                    df[col_name] = df[col_name].fillna(default_val)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error while filling nulls with '{file}' : {e}\")\n",
    "            finally:\n",
    "                df_list.append(df)\n",
    "    \n",
    "    return df_list\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3c9d633",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CODIGO</th>\n",
       "      <th>DISTRITO</th>\n",
       "      <th>DEPARTAMENTO</th>\n",
       "      <th>MUNICIPIO</th>\n",
       "      <th>ESTABLECIMIENTO</th>\n",
       "      <th>DIRECCION</th>\n",
       "      <th>TELEFONO</th>\n",
       "      <th>SUPERVISOR</th>\n",
       "      <th>DIRECTOR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>02-01-0027-46</td>\n",
       "      <td>02-014</td>\n",
       "      <td>EL PROGRESO</td>\n",
       "      <td>GUASTATOYA</td>\n",
       "      <td>INSTITUTO TECNICO INDUSTRIAL MIXTO GUASTATOYA</td>\n",
       "      <td>BARRIO EL CALVARIO</td>\n",
       "      <td>79450881.0</td>\n",
       "      <td>CARLA MARLENY ALDANA RODAS</td>\n",
       "      <td>JOSÉ ARTURO LÓPEZ ORTIZ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>02-01-0028-46</td>\n",
       "      <td>02-014</td>\n",
       "      <td>EL PROGRESO</td>\n",
       "      <td>GUASTATOYA</td>\n",
       "      <td>COLEGIO DE CIENCIAS COMERCIALES EL PROGRESO</td>\n",
       "      <td>BARRIO EL PORVENIR</td>\n",
       "      <td>79451265.0</td>\n",
       "      <td>CARLA MARLENY ALDANA RODAS</td>\n",
       "      <td>DANIEL LÓPEZ SOLÍS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>02-01-0031-46</td>\n",
       "      <td>02-014</td>\n",
       "      <td>EL PROGRESO</td>\n",
       "      <td>GUASTATOYA</td>\n",
       "      <td>INSTITUTO NACIONAL DE EDUCACION DIVERSIFICADA</td>\n",
       "      <td>BARRIO EL PORVENIR</td>\n",
       "      <td>54422753.0</td>\n",
       "      <td>CARLA MARLENY ALDANA RODAS</td>\n",
       "      <td>ILIANA ARACELY LÁZARO HERNÁNDEZ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>02-01-0045-46</td>\n",
       "      <td>02-014</td>\n",
       "      <td>EL PROGRESO</td>\n",
       "      <td>GUASTATOYA</td>\n",
       "      <td>INSTITUTO DE EDUCACIÓN MEDIA POR MADUREZ</td>\n",
       "      <td>BARRIO EL PORVENIR</td>\n",
       "      <td>56719955.0</td>\n",
       "      <td>CARLA MARLENY ALDANA RODAS</td>\n",
       "      <td>NORA REBECA IBAÑEZ MORÁN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>02-01-0049-46</td>\n",
       "      <td>02-021</td>\n",
       "      <td>EL PROGRESO</td>\n",
       "      <td>GUASTATOYA</td>\n",
       "      <td>CENTRO MUNICIPAL DE EDUCACIÓN EXTRAESCOLAR -CEEX-</td>\n",
       "      <td>COLONIA LINDA VISTA</td>\n",
       "      <td>58250827.0</td>\n",
       "      <td>MARTA ELIDA CARIAS HERNANDEZ DE GARCIA</td>\n",
       "      <td>SANDRA PAOLA MORALES GARCÍA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>02-01-0054-46</td>\n",
       "      <td>02-012</td>\n",
       "      <td>EL PROGRESO</td>\n",
       "      <td>GUASTATOYA</td>\n",
       "      <td>COLEGIO EVANGELICO TORRE FUERTE</td>\n",
       "      <td>COLONIA HICHOS, BARRIO EL PORVENIR</td>\n",
       "      <td>79451993.0</td>\n",
       "      <td>CARLOTA EUGENIA ALBUREZ AGUILAR</td>\n",
       "      <td>IRIS ORFELÍ LOAIZA MOSCOSO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>02-01-0061-46</td>\n",
       "      <td>02-021</td>\n",
       "      <td>EL PROGRESO</td>\n",
       "      <td>GUASTATOYA</td>\n",
       "      <td>PROGRAMA NACIONAL DE EDUCACIÓN ALTERNATIVA -PR...</td>\n",
       "      <td>BARRIO LAS JOYAS CEMENTERIO VIEJO</td>\n",
       "      <td>79637575.0</td>\n",
       "      <td>MARTA ELIDA CARIAS HERNANDEZ DE GARCIA</td>\n",
       "      <td>MARTA ELIDA CARIAS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>02-01-0062-46</td>\n",
       "      <td>02-021</td>\n",
       "      <td>EL PROGRESO</td>\n",
       "      <td>GUASTATOYA</td>\n",
       "      <td>CENTRO DE EDUCACIÓN EXTRAESCOLAR -CEEX- DEPART...</td>\n",
       "      <td>BARRIO LAS JOYAS, CEMENTERIO VIEJO</td>\n",
       "      <td>37962236.0</td>\n",
       "      <td>MARTA ELIDA CARIAS HERNANDEZ DE GARCIA</td>\n",
       "      <td>HELLEN CELESTE MARROQUIN CORADO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>02-01-0066-46</td>\n",
       "      <td>02-012</td>\n",
       "      <td>EL PROGRESO</td>\n",
       "      <td>GUASTATOYA</td>\n",
       "      <td>COLEGIO MIXTO INTEGRAL EL PROGRESO</td>\n",
       "      <td>COLONIA HICHOS, BARRIO EL PORVENIR</td>\n",
       "      <td>47406758.0</td>\n",
       "      <td>CARLOTA EUGENIA ALBUREZ AGUILAR</td>\n",
       "      <td>DELFINA MARISOL PAZOS RAMOS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          CODIGO DISTRITO DEPARTAMENTO   MUNICIPIO  \\\n",
       "0  02-01-0027-46   02-014  EL PROGRESO  GUASTATOYA   \n",
       "1  02-01-0028-46   02-014  EL PROGRESO  GUASTATOYA   \n",
       "2  02-01-0031-46   02-014  EL PROGRESO  GUASTATOYA   \n",
       "3  02-01-0045-46   02-014  EL PROGRESO  GUASTATOYA   \n",
       "4  02-01-0049-46   02-021  EL PROGRESO  GUASTATOYA   \n",
       "5  02-01-0054-46   02-012  EL PROGRESO  GUASTATOYA   \n",
       "6  02-01-0061-46   02-021  EL PROGRESO  GUASTATOYA   \n",
       "7  02-01-0062-46   02-021  EL PROGRESO  GUASTATOYA   \n",
       "8  02-01-0066-46   02-012  EL PROGRESO  GUASTATOYA   \n",
       "\n",
       "                                     ESTABLECIMIENTO  \\\n",
       "0      INSTITUTO TECNICO INDUSTRIAL MIXTO GUASTATOYA   \n",
       "1        COLEGIO DE CIENCIAS COMERCIALES EL PROGRESO   \n",
       "2      INSTITUTO NACIONAL DE EDUCACION DIVERSIFICADA   \n",
       "3           INSTITUTO DE EDUCACIÓN MEDIA POR MADUREZ   \n",
       "4  CENTRO MUNICIPAL DE EDUCACIÓN EXTRAESCOLAR -CEEX-   \n",
       "5                    COLEGIO EVANGELICO TORRE FUERTE   \n",
       "6  PROGRAMA NACIONAL DE EDUCACIÓN ALTERNATIVA -PR...   \n",
       "7  CENTRO DE EDUCACIÓN EXTRAESCOLAR -CEEX- DEPART...   \n",
       "8                 COLEGIO MIXTO INTEGRAL EL PROGRESO   \n",
       "\n",
       "                            DIRECCION    TELEFONO  \\\n",
       "0                  BARRIO EL CALVARIO  79450881.0   \n",
       "1                  BARRIO EL PORVENIR  79451265.0   \n",
       "2                  BARRIO EL PORVENIR  54422753.0   \n",
       "3                  BARRIO EL PORVENIR  56719955.0   \n",
       "4                 COLONIA LINDA VISTA  58250827.0   \n",
       "5  COLONIA HICHOS, BARRIO EL PORVENIR  79451993.0   \n",
       "6   BARRIO LAS JOYAS CEMENTERIO VIEJO  79637575.0   \n",
       "7  BARRIO LAS JOYAS, CEMENTERIO VIEJO  37962236.0   \n",
       "8  COLONIA HICHOS, BARRIO EL PORVENIR  47406758.0   \n",
       "\n",
       "                               SUPERVISOR                         DIRECTOR  \n",
       "0              CARLA MARLENY ALDANA RODAS          JOSÉ ARTURO LÓPEZ ORTIZ  \n",
       "1              CARLA MARLENY ALDANA RODAS               DANIEL LÓPEZ SOLÍS  \n",
       "2              CARLA MARLENY ALDANA RODAS  ILIANA ARACELY LÁZARO HERNÁNDEZ  \n",
       "3              CARLA MARLENY ALDANA RODAS         NORA REBECA IBAÑEZ MORÁN  \n",
       "4  MARTA ELIDA CARIAS HERNANDEZ DE GARCIA      SANDRA PAOLA MORALES GARCÍA  \n",
       "5         CARLOTA EUGENIA ALBUREZ AGUILAR       IRIS ORFELÍ LOAIZA MOSCOSO  \n",
       "6  MARTA ELIDA CARIAS HERNANDEZ DE GARCIA               MARTA ELIDA CARIAS  \n",
       "7  MARTA ELIDA CARIAS HERNANDEZ DE GARCIA  HELLEN CELESTE MARROQUIN CORADO  \n",
       "8         CARLOTA EUGENIA ALBUREZ AGUILAR      DELFINA MARISOL PAZOS RAMOS  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_list: list[pd.DataFrame] = fill_nulls_with_values(path=dataset_path, value_dict=missing_data_dict, columns_to_clean=['ESTABLECIMIENTO'])\n",
    "df_list[0].iloc[:9, :9]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f14f51",
   "metadata": {},
   "source": [
    "### Transformar las palabras a forma estándar; quitar tildes y diéresis\n",
    "\n",
    "Una vez que hemos llenado los valores faltantes, es crucial estandarizar el formato del texto en todo el conjunto de datos. Esto es especialmente importante para garantizar consistencia y evitar problemas en análisis posteriores donde palabras iguales pero escritas de forma diferente sean tratadas como distintas.\n",
    "\n",
    "**Objetivos de esta estandarización:**\n",
    "\n",
    "1. **Eliminar acentos y caracteres especiales**: Normalizar palabras como \"José\" → \"Jose\", \"Quiché\" → \"Quiche\"\n",
    "2. **Unificar formatos por tipo de campo**:\n",
    "   - **Establecimientos**: TODO EN MAYÚSCULAS para consistencia\n",
    "   - **Nombres propios** (Director/Supervisor): Formato Título (Primera Letra Mayúscula)\n",
    "   - **Direcciones**: Expandir abreviaciones (Ave → Avenida, Km → Kilometro)\n",
    "   - **Teléfonos**: Formato uniforme de 8 dígitos sin separadores\n",
    "3. **Limpiar espacios y puntuación**: Eliminar espacios múltiples y comillas redundantes\n",
    "\n",
    "**Transformaciones específicas aplicadas:**\n",
    "\n",
    "- **Normalización Unicode**: Separar caracteres base de sus acentos y eliminar los acentos\n",
    "- **Estandarización de direcciones**: \"5a. ave 1-23 zona 4\" → \"5A AVENIDA 1-23 ZONA 4\"\n",
    "- **Formato de teléfonos**: \"7794-5104\" → \"77945104\", \"79529782.0\" → \"79529782\"\n",
    "- **Nombres propios**: \"JOSÉ MARÍA HERNÁNDEZ\" → \"Jose Maria Hernandez\"\n",
    "- **Establecimientos**: \"Colegio San José\" → \"COLEGIO SAN JOSE\"\n",
    "\n",
    "Esta estandarización es fundamental para:\n",
    "- Evitar duplicados por diferencias tipográficas\n",
    "- Facilitar búsquedas y filtros posteriores  \n",
    "- Garantizar consistencia en el conjunto de datos final\n",
    "- Preparar los datos para análisis automatizados\n",
    "\n",
    "**Proceso:** Se aplicará la estandarización a todos los DataFrames que ya contienen los valores nulos rellenados, manteniendo la integridad de los datos mientras se mejora su formato y consistencia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c8488e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aplicando estandarización de texto...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Estandarizando DataFrames: 100%|██████████| 85/85 [00:01<00:00, 44.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Estandarización completada para 85 DataFrames\n",
      "\n",
      "📊 RESUMEN:\n",
      "• Total de archivos estandarizados: 85\n",
      "• Total de registros procesados: 38057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def remove_accents_and_special_chars(text: str) -> str:\n",
    "    \"\"\"Remueve tildes, diéresis y normaliza caracteres especiales\"\"\"\n",
    "    if pd.isnull(text):\n",
    "        return text\n",
    "    \n",
    "    text = str(text).strip()\n",
    "    # Normalizar unicode y remover acentos\n",
    "    text = unicodedata.normalize('NFD', text)\n",
    "    text = ''.join(char for char in text if unicodedata.category(char) != 'Mn')\n",
    "    return text\n",
    "\n",
    "def standardize_phone_number(phone: str) -> str:\n",
    "    \"\"\"Estandariza números telefónicos a formato consistente (8 dígitos)\"\"\"\n",
    "    if pd.isnull(phone) or phone == \"00000000\":\n",
    "        return \"00000000\"\n",
    "    \n",
    "    phone_str = str(phone).strip()\n",
    "    digits_only = re.sub(r'\\D', '', phone_str)\n",
    "    \n",
    "    if len(digits_only) == 8:\n",
    "        return digits_only\n",
    "    elif len(digits_only) > 8:\n",
    "        return digits_only[-8:]\n",
    "    elif len(digits_only) < 8 and len(digits_only) > 0:\n",
    "        return digits_only.zfill(8)\n",
    "    else:\n",
    "        return \"00000000\"\n",
    "\n",
    "def standardize_address(address: str) -> str:\n",
    "    \"\"\"Estandariza formato de direcciones\"\"\"\n",
    "    if pd.isnull(address) or address == \"DESCONOCIDO\":\n",
    "        return \"DESCONOCIDO\"\n",
    "    \n",
    "    address = str(address).strip().upper()\n",
    "    \n",
    "    # Estandarizar abreviaciones comunes\n",
    "    replacements = {\n",
    "        r'\\bAV\\b\\.?': 'AVENIDA',\n",
    "        r'\\bAVE\\b\\.?': 'AVENIDA', \n",
    "        r'\\bCALL\\b\\.?': 'CALLE',\n",
    "        r'\\bKM\\b\\.?': 'KILOMETRO',\n",
    "        r'\\bZON\\b\\.?': 'ZONA',\n",
    "        r'\\bALD\\b\\.?': 'ALDEA'\n",
    "    }\n",
    "    \n",
    "    for pattern, replacement in replacements.items():\n",
    "        address = re.sub(pattern, replacement, address)\n",
    "    \n",
    "    address = re.sub(r'\\s+', ' ', address)\n",
    "    return address.strip()\n",
    "\n",
    "def comprehensive_text_cleaning(text: str, field_type: str = 'general') -> str:\n",
    "    \"\"\"Aplica limpieza comprehensiva según el tipo de campo\"\"\"\n",
    "    if pd.isnull(text):\n",
    "        return text\n",
    "    \n",
    "    # Limpieza básica\n",
    "    text = str(text).strip()\n",
    "    text = text.replace('\"\"', '\"').replace(\"''\", \"'\")\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = remove_accents_and_special_chars(text)\n",
    "    \n",
    "    # Aplicar estandarización específica\n",
    "    if field_type == 'phone':\n",
    "        return standardize_phone_number(text)\n",
    "    elif field_type == 'address':\n",
    "        return standardize_address(text)\n",
    "    elif field_type == 'name':\n",
    "        # Para nombres: Primera letra mayúscula, resto minúscula\n",
    "        return ' '.join(word.capitalize() for word in text.split())\n",
    "    elif field_type == 'establishment':\n",
    "        return text.upper()\n",
    "    else:\n",
    "        return text.upper()\n",
    "\n",
    "def apply_standardization_to_dataframes(df_list: list[pd.DataFrame]) -> list[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Aplica estandarización de texto a una lista de DataFrames\n",
    "    \"\"\"\n",
    "    standardized_dfs = []\n",
    "    \n",
    "    # Definir qué tipo de limpieza aplicar a cada columna\n",
    "    field_mappings = {\n",
    "        'TELEFONO': 'phone',\n",
    "        'DIRECCION': 'address', \n",
    "        'DIRECTOR': 'name',\n",
    "        'SUPERVISOR': 'name',\n",
    "        'ESTABLECIMIENTO': 'establishment',\n",
    "        'DEPARTAMENTO': 'general',\n",
    "        'MUNICIPIO': 'general',\n",
    "        'DISTRITO': 'general',\n",
    "        'NIVEL': 'general',\n",
    "        'SECTOR': 'general',\n",
    "        'AREA': 'general',\n",
    "        'STATUS': 'general',\n",
    "        'MODALIDAD': 'general',\n",
    "        'JORNADA': 'general',\n",
    "        'PLAN': 'general',\n",
    "        'DEPARTAMENTAL': 'general',\n",
    "        'CODIGO': 'general'\n",
    "    }\n",
    "    \n",
    "    print(\"Aplicando estandarización de texto...\")\n",
    "    \n",
    "    for i, df in enumerate(tqdm(df_list, desc=\"Estandarizando DataFrames\")):\n",
    "        df_clean = df.copy()\n",
    "        \n",
    "        # Aplicar limpieza específica a cada columna\n",
    "        for column, field_type in field_mappings.items():\n",
    "            if column in df_clean.columns:\n",
    "                df_clean[column] = df_clean[column].apply(\n",
    "                    lambda x: comprehensive_text_cleaning(x, field_type)\n",
    "                )\n",
    "        \n",
    "        standardized_dfs.append(df_clean)\n",
    "    \n",
    "    print(f\"✅ Estandarización completada para {len(standardized_dfs)} DataFrames\")\n",
    "    return standardized_dfs\n",
    "\n",
    "# Aplicar estandarización a los DataFrames que ya tienes\n",
    "df_list_standardized = apply_standardization_to_dataframes(df_list)\n",
    "\n",
    "print(f\"\\n📊 RESUMEN:\")\n",
    "print(f\"• Total de archivos estandarizados: {len(df_list_standardized)}\")\n",
    "print(f\"• Total de registros procesados: {sum(len(df) for df in df_list_standardized)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b3f199",
   "metadata": {},
   "source": [
    "### Concatenar todo a un mismo .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "177b6c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86699034",
   "metadata": {},
   "source": [
    "### Estandarizar el csv conjunto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2aec070d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
