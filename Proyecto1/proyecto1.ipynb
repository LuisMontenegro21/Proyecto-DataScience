{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0894d108",
   "metadata": {},
   "source": [
    "# Primera entrega\n",
    "Luis Montenegro - 21699<br>\n",
    "Javier Prado - 21486<br>\n",
    "Bryan España<br>\n",
    "Ángel Herrarte<br>\n",
    "\n",
    "\n",
    "## Limpieza de datos\n",
    "En esta entrega se hará una recolección y limpieza de datos solamente. No habrá modelado ni análisis. \n",
    "<br>Solamente velar por la integridad, coherencia y cohesión del conjunto de datos.facilidad de manejo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff6290c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "ERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements.txt'\n"
     ]
    }
   ],
   "source": [
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fa43cd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a3fcc9",
   "metadata": {},
   "source": [
    "### Conversión de .xls a .csv\n",
    "\n",
    "Debido a que los archivos crudos descargados desde la página del Mineduc vienen en formato .xls formateado como .html tenemos que hacer cambio de eso.<br>\n",
    "Esto por múltiples razones:\n",
    "- Mejorar la estructura de los datos.\n",
    "- Preservar solamente la información requerida (los .xls almaceban más páginas e información irrelevante)\n",
    "- Mayor falicidad de usar y obtener los datos si están en formato delimitado por comas.\n",
    "- Tener datos más ligeros ya que no acarreamos con mucha información innecesaria. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "456d91fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_html_to_csv(input_path: str, output_path: str=None, tables_to_convert: list[int] = [9]) -> None:\n",
    "    '''\n",
    "    Turns .html files into .csv files\n",
    "\n",
    "    Params:\n",
    "        input_path: where the .html files are stored\n",
    "        output_path: where the .csv will be stored\n",
    "    Returns:\n",
    "        out: None\n",
    "    '''\n",
    "    if output_path is None:\n",
    "        output_path = input_path \n",
    "    else:\n",
    "        os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "    for filename in tqdm(os.listdir(input_path), desc='Converting Files'):\n",
    "        if filename.endswith('.xls'):\n",
    "            html_path:str = os.path.join(input_path, filename)\n",
    "            base_name:str = os.path.splitext(filename)[0]\n",
    "            # try to read the excel and copy it into a .csv file\n",
    "            try: \n",
    "                tables:list[pd.DataFrame] = pd.read_html(html_path)\n",
    "                # if no tables, skip\n",
    "                if not tables:\n",
    "                    print(f\"No tables found in '{filename}'\")\n",
    "                    continue\n",
    "\n",
    "                # convert each of the tables selected to .csv\n",
    "                for i, df in enumerate(tables):\n",
    "                    if i in tables_to_convert:\n",
    "                        csv_filename = f\"{base_name}_table{i}.csv\"\n",
    "                        csv_path = os.path.join(output_path, csv_filename)\n",
    "                        df.to_csv(csv_path, index=False)\n",
    "                        print(f\"Converted '{filename}' to '{csv_filename}'\") # print all of the transformations of the .xls to its respective .csv files\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file '{filename}' : {e}\")\n",
    "\n",
    "def remove_files(path: str, extension: str) -> None:\n",
    "    '''\n",
    "    Removes all files that have a certain extension\n",
    "    Input:\n",
    "        path: folder path where .xls to be removed are stored\n",
    "    '''\n",
    "    file_list: list[str] = os.listdir(path)\n",
    "    if not file_list:\n",
    "        print(f\"No files removed since there was none found: '{path}'\")\n",
    "        return\n",
    "\n",
    "    for filename in file_list:\n",
    "    \n",
    "        if filename.endswith(extension):\n",
    "            try:\n",
    "                xls_path = os.path.join(path, filename)\n",
    "                os.remove(xls_path)\n",
    "            except Exception as e:\n",
    "                print(f\"Error removing file '{filename}' : {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f249f5f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting Files: 0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "# start by turning .xls files to .csv files\n",
    "transform_html_to_csv(input_path=\"../Dataset_raw\", output_path=\"../Dataset_cleaned\", tables_to_convert=[9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b81a4e90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No files removed since there was none found: '../Dataset_raw'\n"
     ]
    }
   ],
   "source": [
    "# remove .xls files if necessary\n",
    "remove_files(path=\"../Dataset_raw\", extension='.xls')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65016283",
   "metadata": {},
   "source": [
    "### Estandarizar la estructura de los .csv\n",
    "Es importante que los .csv con los que estamos trabajando tengan una misma estructura. Por lo tanto, debemos asegurarnos que todos sean iguales. <br>\n",
    "Esto lo hacemos con la finalidad de evitar errores al limpiar los datos más adelante o al concatenarlos al final. \n",
    "\n",
    "Queremos que todos los .csv tengan 17 títulos de columnas en orden tipo:<br>\n",
    "CODIGO,DISTRITO,DEPARTAMENTO,MUNICIPIO,ESTABLECIMIENTO,DIRECCION,TELEFONO,SUPERVISOR,DIRECTOR,NIVEL,SECTOR,AREA,STATUS,MODALIDAD,JORNADA,PLAN,DEPARTAMENTAL<br>\n",
    "\n",
    "Otra cosa a considerar es eliminar la primera fila y reajustar índices. Debido a que la primera fila son puros números en vez de los nombres de las columnas. Al igual que debemos\n",
    "eliminar la última fila que es todos nulos. \n",
    "\n",
    "Para los que tenían error de formato o formato diferente solamente copiamos los datos en un .csv ya que usualmente eso se debe a que solamente había 1 establecimiento encontrado.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3558893f",
   "metadata": {},
   "outputs": [],
   "source": [
    "COLUMNS = [\"CODIGO\",\"DISTRITO\",\"DEPARTAMENTO\",\"MUNICIPIO\",\"ESTABLECIMIENTO\",\"DIRECCION\",\"TELEFONO\",\"SUPERVISOR\",\"DIRECTOR\",\"NIVEL\",\"SECTOR\",\"AREA\",\"STATUS\",\"MODALIDAD\",\"JORNADA\",\"PLAN\",\"DEPARTAMENTAL\"]\n",
    "\n",
    "def clean_headers_and_trailers(path: str) -> None:\n",
    "    '''\n",
    "    Removes first row and reindexes the data within.\n",
    "    Also removes the last row that contains only nulls\n",
    "    '''\n",
    "    for file in tqdm(os.listdir(path), desc=\"Converting files\"):\n",
    "        if file.endswith('.csv'):\n",
    "            try:\n",
    "                f_path = os.path.join(path, file) # path to the file\n",
    "                df: pd.DataFrame = pd.read_csv(f_path)\n",
    "                \n",
    "                # drop first row and reindex if the first row are digits and not columns\n",
    "                if all(col.strip().isdigit() for col in df.columns):\n",
    "                    df.columns = df.iloc[0]\n",
    "                    df = df[1:].reset_index(drop=True)\n",
    "\n",
    "                # remove last row if only nulls are found\n",
    "                if df.tail(1).isnull().all(axis=1).iloc[0]:\n",
    "                    df = df.iloc[:-1]\n",
    "                    print(f\"Removed last row: {file}\")\n",
    "                \n",
    "                df.to_csv(f_path, index=False)  \n",
    "                \n",
    "                print(f\"Finished: {file}\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing '{file}': {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99e3f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean headers and trailers\n",
    "# clean_headers_and_trailers(path = \"../Dataset_cleaned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7e543e",
   "metadata": {},
   "source": [
    "### Plan para limpieza de datos\n",
    "Posteriormente a la transformación de formato y filtración a solo datos crudos de nuestro interés, procederemos a limpiar los datos en sí. <br>\n",
    "1. Verificaremos si existe datos NA o Null en los archivos. Llenaremos los datos faltantes con datos artificiales ya que no podemos eliminar ningún registro <br>\n",
    "debido a que si eliminamos alguno, sería una sede faltante. Cosa que afectaría de gran manera el conjunto de datos.\n",
    "2. Homogenizaremos la información. Es decir, que todos los archivos tengan el mismo formato i.e. todos los nombres estén escritos igual, los apellidos que tienen tildes se escriban igual en cada archivo,\n",
    "que las palabras vengan o solo en mayúsculas, solo en minúsculas, etc.\n",
    "3. Identificaremos las columnas que más trabajo de reajuste necesiten.\n",
    "- Nombres (Supervisor, Director, Establecimiento, Sector): debido a que puede que exista apellidos iguales pero escritos distinto y eso genere problemas a la hora de verlos. Digamos, puede que Hernández aparezca con tilde en unos registros y en otros no. También es bueno verificar que estén escritos todos o en mayúsculas o en minúsculas (o todos iguales), ya que digamos, si tenemos Santa rosa en un registro y Santa Rosa en otro, a la hora de hacer un encoding estos resultarán con dos valores distintos. \n",
    "- Dirección: Asegurarnos que las direcciones lleven una estructura similar.\n",
    "- Códigos: Verificar que los códigos de los registros sean únicos y evitar tener repetidos.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6208bf",
   "metadata": {},
   "source": [
    "El conjunto de datos corresponde a los establecimientos educativos de Guatemala que llegan hasta el nivel diversificado. Está organizado en 85 archivos CSV, uno por cada departamento y municipio del país.\n",
    "\n",
    "Cada archivo contiene información detallada de cada establecimiento, como el nombre del centro, su ubicación, datos de contacto, modalidad de enseñanza y otros datos administrativos.\n",
    "\n",
    "\n",
    "Los archivos csv estan clasificados por departamentos de Guatemala. Las variables que contiene esta dataset son las siguientes sumando un total de 17 variables a analizar.\n",
    "\n",
    "- CODIGO: código único del establecimiento\n",
    "\n",
    "- DISTRITO: distrito educativo\n",
    "\n",
    "- DEPARTAMENTO: nombre del departamento\n",
    "\n",
    "- MUNICIPIO: municipio donde se ubica\n",
    "\n",
    "- ESTABLECIMIENTO: nombre del centro educativo\n",
    "\n",
    "- DIRECCION: dirección física\n",
    "\n",
    "- TELEFONO: número de contacto\n",
    "\n",
    "- SUPERVISOR: nombre del supervisor\n",
    "\n",
    "- DIRECTOR: nombre del director\n",
    "\n",
    "- NIVEL: nivel educativo (ej. Básico, Diversificado)\n",
    "\n",
    "- SECTOR: sector oficial o privado\n",
    "\n",
    "- AREA: área urbana o rural\n",
    "\n",
    "- STATUS: estado del centro (ej. Abierta)\n",
    "\n",
    "- MODALIDAD: modalidad lingüística (ej. Monolingüe, Bilingüe)\n",
    "\n",
    "- JORNADA: jornada de estudio (ej. Matutina, Vespertina)\n",
    "\n",
    "- PLAN: plan educativo (ej. Diario, Fin de semana)\n",
    "\n",
    "- DEPARTAMENTAL: nombre del departamento de adscripción"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0657006d",
   "metadata": {},
   "source": [
    "### Encontrar Null o NA en cualesquiera archivos\n",
    "Comenzaremos viendo si existe cualesquiera archivos con datos faltantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "37e5db5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path: str = \"../Dataset_cleaned\" # define folder path as a variable for easier handling\n",
    "report_path: str = \"../Data_null_report\"\n",
    "\n",
    "def get_null_stats(path:str) -> None:\n",
    "        '''\n",
    "        Prints the columns with most null counts and the files with most nulls\n",
    "        '''\n",
    "        df: pd.DataFrame = pd.read_csv(path)\n",
    "        print(\"\\n Top columns with the most nulls:\")\n",
    "        top_columns = df.groupby('column')['nulls'].sum().sort_values(ascending=False).head(10)\n",
    "        print(top_columns)\n",
    "\n",
    "        print(\"\\n Files with the most nulls:\")\n",
    "        top_files = df.groupby('file_name')['nulls'].sum().sort_values(ascending=False).head(10)\n",
    "        print(top_files)\n",
    "\n",
    "def count_null_instances(path:str, report_name:str = \"null_report.csv\") -> None:\n",
    "    '''\n",
    "    Counts how many missing instances are found per column per .csv and saves it into a .csv\n",
    "    '''\n",
    "    files: list[str] = os.listdir(path=path)\n",
    "    report_file: str = os.path.join(report_path, report_name)\n",
    "    os.makedirs(name=report_path, exist_ok=True) # make a new directory to save this information\n",
    "    data:list[dict] = []\n",
    "    for file in tqdm(files, desc=\"Counting nulls..\"):\n",
    "        file_path:str = os.path.join(path, file)\n",
    "        try:\n",
    "            df: pd.DataFrame = pd.read_csv(file_path) \n",
    "            null_counts = df.isnull().sum() # register nulls found in the df per column\n",
    "            for col, count in null_counts.items():\n",
    "                data.append({\n",
    "                    \"file_name\": file,\n",
    "                    \"column\": col,\n",
    "                    \"nulls\": count\n",
    "                })\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file} : {e}\")\n",
    "    \n",
    "    report_df: pd.DataFrame = pd.DataFrame(data=data)\n",
    "    report_df.to_csv(report_file, index=False) # not necessary to save it as a .csv\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dca41853",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Counting nulls..: 100%|██████████| 85/85 [00:00<00:00, 429.89it/s]\n"
     ]
    }
   ],
   "source": [
    "count_null_instances(path=dataset_path) # count nulls found per column in each .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6e9e502d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Top columns with the most nulls:\n",
      "column\n",
      "TELEFONO           4320\n",
      "DIRECTOR            116\n",
      "DIRECCION            23\n",
      "AREA                  0\n",
      "DEPARTAMENTAL         0\n",
      "DEPARTAMENTO          0\n",
      "DISTRITO              0\n",
      "ESTABLECIMIENTO       0\n",
      "CODIGO                0\n",
      "JORNADA               0\n",
      "Name: nulls, dtype: int64\n",
      "\n",
      " Files with the most nulls:\n",
      "file_name\n",
      "alta_verapaz_primaria_table9.csv        633\n",
      "alta_verapaz_preprimaria_table9.csv     376\n",
      "quiche_primaria_table9.csv              334\n",
      "peten_primaria_table9.csv               311\n",
      "jutiapa_primaria_table9.csv             277\n",
      "huehuetenango_primaria_table9.csv       258\n",
      "san_marcos_primaria_table9.csv          225\n",
      "santa_rosa_primaria_table9.csv          225\n",
      "baja_verapaz_primaria_table9.csv        166\n",
      "huehuetenango_preprimaria_table9.csv    138\n",
      "Name: nulls, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "get_null_stats(path=\"../Data_null_report/null_report.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e19e2a",
   "metadata": {},
   "source": [
    "Como podemos observar, las columnas con mayor recuento de nulos son de TELEFONO, DIRECTOR y DIRECCION. Mientras que los archivos con más nulos son altaverapaz con primaria y preprimaria. \n",
    "Por lo tanto, lo que debemos hacer con esto es darles un valor a los nulos. Por ejemplo, un número predeterminado cuando no hay, o un texto único para cuando no haya nombres o direciones, etc. <br>\n",
    "De esta manera no eliminamos ni afectamos el conjunto de datos y aún logramos clasificarlos dentro de su propia categoría."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61ad13a",
   "metadata": {},
   "source": [
    "### Verificación de tipos de datos y consistencia entre archivos\n",
    "Antes de comenzar a llenar los campos vacíos, debemos asegurarnos que los campos que estamos llenando contengan los mismos tipos. <br>\n",
    "Es decir, que no tengamos archivos .csv que tengan tipos int64 y otros object bajo la misma columna. Ya que esto generará problemas a la hora<br>\n",
    "de trabajar los datos y puede generar dificultades de manejo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "838a140d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check datatypes for each column within a file\n",
    "def check_datatypes(file: str, examples: bool = False) -> tuple[defaultdict[set], defaultdict[dict]]:\n",
    "    df: pd.DataFrame = pd.read_csv(file)\n",
    "    type_map = defaultdict(set)\n",
    "    examples_map = defaultdict(dict)\n",
    "    for col in df.columns:\n",
    "        dtype = str(df[col].dtype)\n",
    "        type_map[col].add(dtype)\n",
    "        \n",
    "        if examples:\n",
    "            for val in df[col]:\n",
    "                if pd.notnull(val):\n",
    "                    val_type = str(pd.Series([val]).dtype)\n",
    "                    if val not in examples_map[col]:\n",
    "                        examples_map[col][val_type] = val\n",
    "                    break\n",
    "    return type_map, examples_map\n",
    "\n",
    "def check_datatype_differences(path: str, examples: bool = False) -> None:\n",
    "    '''\n",
    "    Checks if there is any difference between columns' datatypes across all files\n",
    "    '''\n",
    "    \n",
    "    global_type_map = defaultdict(set)\n",
    "    global_examples_map = defaultdict(dict)\n",
    "    for file in os.listdir(path):\n",
    "        if file.endswith('.csv'):\n",
    "            try: \n",
    "                f_path: str = os.path.join(path, file)\n",
    "                type_map, examples_map = check_datatypes(f_path, examples=examples)\n",
    "                for col, types in type_map.items():\n",
    "                    global_type_map[col].update(types)\n",
    "                \n",
    "                if examples:\n",
    "                    for col, example_dict in examples_map.items():\n",
    "                        for dtype, val in example_dict.items():\n",
    "                            if dtype not in global_examples_map[col]:\n",
    "                                global_examples_map[col][dtype] = val\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Unexpected error on file '{file}': {e}\")\n",
    "\n",
    "    print(f\"Columns and their types found\")\n",
    "    for col, types in global_type_map.items():\n",
    "        print(f\"- {col} : {types}\")\n",
    "        if examples and len(types) > 1:\n",
    "            for dtype, value in global_examples_map[col].items():\n",
    "                print(f\"{dtype} : {repr(value)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "85f09562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns and their types found\n",
      "- CODIGO : {'object'}\n",
      "- DISTRITO : {'object'}\n",
      "- DEPARTAMENTO : {'object'}\n",
      "- MUNICIPIO : {'object'}\n",
      "- ESTABLECIMIENTO : {'object'}\n",
      "- DIRECCION : {'object'}\n",
      "- TELEFONO : {'float64', 'object', 'int64'}\n",
      "object : '57101061'\n",
      "float64 : 51904421.0\n",
      "int64 : 77641157\n",
      "- SUPERVISOR : {'object'}\n",
      "- DIRECTOR : {'object'}\n",
      "- NIVEL : {'object'}\n",
      "- SECTOR : {'object'}\n",
      "- AREA : {'object'}\n",
      "- STATUS : {'object'}\n",
      "- MODALIDAD : {'object'}\n",
      "- JORNADA : {'object'}\n",
      "- PLAN : {'object'}\n",
      "- DEPARTAMENTAL : {'object'}\n"
     ]
    }
   ],
   "source": [
    "check_datatype_differences(path=dataset_path, examples=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37199741",
   "metadata": {},
   "source": [
    "Como podemos observar arriba, tenemos que los tipos encontrados son en su mayoría object, pero la columna TELEFONO posee tanto object como int64 y float 64. <br>\n",
    "Eso es un problema debido a que no podemos manejar esos 3 tipos a lo largo y ancho del conjunto de datos. Por lo que será necesario indagar a fondo de por qué aparece\n",
    "esos tipos en el área de teléfono. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075debfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO change datatypes to appropiate types depending on the columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9067793d",
   "metadata": {},
   "source": [
    "### Asignar valores a los datos inexistentes o nulos\n",
    "Ahora lo que haremos, como se dijo previamente, es asignarle valores a las celdas nulas. <br>\n",
    "Los siguientes valores predeterminados para las celdas nulas serán los siguientes:<br>\n",
    "\n",
    "DIRECCION : DESCONOCIDO<br>\n",
    "TELEFONO : 00000000<br>\n",
    "DIRECTOR : DESCONOCIDO<br>\n",
    "\n",
    "Pero primero observaremos los tipos de cada columna para tener mejor definición de qué hay en cada una y que no exista<br>\n",
    "discrepancia entre los tipos a lo largo de las columnas y archivos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbdc1bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill missing data with predetermined values\n",
    "val_dict: dict = {\"DIRECCION\":\"DESCONOCIDO\", \"TELEFONO\": 00000000, \"DIRECTOR\": \"DESCONOCIDO\"} \n",
    "\n",
    "def fill_nulls_with_values(path:str, value_dict: dict) -> None:\n",
    "    '''\n",
    "    Changes null instances within columns and fills them with predetermined values\n",
    "    '''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
